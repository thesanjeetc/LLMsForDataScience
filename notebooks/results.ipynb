{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/root/FYP\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils import get_baseline_results\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name_map = {\n",
    "    \"arcade.vanilla-FS\": \"Baseline Vanilla + FS\",\n",
    "    \"arcade.CoT-FS\": \"Baseline CoT + FS\",\n",
    "    \"arcade.CoT-FS+EXP\": \"[+] Explanations\",\n",
    "    \"fyp.vanilla\": \"Vanilla\",\n",
    "    \"fyp.DFS\": \"[+] DataFrames\",\n",
    "    \"fyp.DFS+OUT\": \"[+] Execution Outputs\",\n",
    "    \"fyp.DFS+OUT+FS\": \"[+] Exemplars (ARCADE)\",\n",
    "    \"fyp.DFS+OUT+FS+SCH\": \"[+] Schema Information\",\n",
    "    \"fyp.CoT-DFS+OUT\": \"CoT\",\n",
    "    \"fyp.CoT-DFS+OUT+SCH\": \"[+] Schema Informationn\",\n",
    "    \"fyp.CoT-FS+DFS+OUT+SCH\": \"[+] Exemplars (Custom)\",\n",
    "    \"fyp.CoT+2S-DFS+OUT+SCH\": \"CoT + 2 Step\",\n",
    "    \"fyp.CoT+2S+ERR-DFS+OUT+SCH\": \"[+] Errors\",\n",
    "    \"fyp.CoT+2S+ERR+RES-DFS+OUT+SCH\": \"[+] Outputs\",\n",
    "    \n",
    "    # \"fyp.CoT-aDFS+OUT\": \"APP DFS OUT\",\n",
    "    # \"fyp.CoT-SCH\": \"SCHEMA ONLY\",\n",
    "    # \"fyp.DFS+OUT+FS(AST)\": \"AST EXP\",\n",
    "    # \"fyp.DFS+OUT+SCH+FS(AST)\": \"AST + SCH\"\n",
    "    # \"fyp.CoT-DFS+OUT+SCH-[pass@30]\": \"pass30\",\n",
    "    # \"fyp.CoT-DFS+OUT+SCH-[T]\": \"pass30 + T\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS_ROOT = \"experiments\"\n",
    "\n",
    "def aggregate_results():\n",
    "    results = []\n",
    "    for rootdir, dirs, files in os.walk(EXPERIMENTS_ROOT):\n",
    "        for subdir in dirs:\n",
    "            expdir = os.path.join(rootdir, subdir)\n",
    "            results_path = os.path.join(expdir, \"results.json\")\n",
    "            if os.path.exists(results_path):\n",
    "                with open(results_path, \"r\") as f:\n",
    "                    data = json.loads(f.read())\n",
    "                results.extend(list(map(lambda e: e | {\"experiment_name\": str(subdir)}, data)))\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df = aggregate_results()\n",
    "\n",
    "def extract_single_temp(r):\n",
    "    t = 0.6\n",
    "    i = int(t / 0.2)\n",
    "    n = 5\n",
    "    if r[\"experiment_name\"] == \"vanilla_raw_notebook\":\n",
    "        return r[\"predictions\"][i*n:(i+1)*n] \n",
    "    return r[\"predictions\"]\n",
    "\n",
    "df[\"predictions\"] = df.apply(extract_single_temp, axis=1)\n",
    "\n",
    "# experiments = [\"vanilla_raw_notebook\", \"chain_of_thought\", \"chain_of_thought_explanations\", \"fyp.vanilla\", \"fyp.with_dataframes\", \"fyp.with_outputs\", \"fyp.with_dataframes_and_outputs\", \"fyp.with_dataframes_at_creation\", \"fyp.with_created_dataframes_new_prompt\", \"fyp.with_created_dataframes_outputs_new_prompt\", \"fyp.with_created_dataframes_outputs_promptv3\", \"fyp.cot_dataframes_outputs_v2_exemplars_df_info\"]\n",
    "experiments = [\"arcade.vanilla-FS\", \"arcade.CoT-FS\", \"arcade.CoT-FS+EXP\", \"fyp.vanilla\", \"fyp.DFS\", \"fyp.DFS+OUT\", \"fyp.DFS+OUT+FS\", \"fyp.DFS+OUT+FS+SCH\", \"fyp.CoT-DFS+OUT\", \"fyp.CoT-DFS+OUT+SCH\", \"fyp.CoT-FS+DFS+OUT+SCH\", \"fyp.CoT+2S-DFS+OUT+SCH\", \"fyp.CoT+2S+ERR-DFS+OUT+SCH\", \"fyp.CoT+2S+ERR+RES-DFS+OUT+SCH\"]\n",
    "               \n",
    "            #    , \"fyp.CoT-SCH\"] \n",
    "\n",
    "# , \"fyp.CoT-aDFS+OUT\",\"fyp.DFS+OUT+FS(AST)\", \"fyp.DFS+OUT+SCH+FS(AST)\"] # \"fyp.CoT-DFS+OUT+SCH-[pass@30]\", \"fyp.CoT-DFS+OUT+SCH-[T]\"] # \"fyp.SPLIT-DFS+OUT+FS+SCH\"\n",
    "# experiments = [\"fyp.CoT-DFS+OUT+SCH\", \"fyp.CoT-FS+DFS+OUT+SCH\", \"fyp.CoT+2S-DFS+OUT+SCH\", \"fyp.CoT+2S+ERR-DFS+OUT+SCH\", \"fyp.CoT+2S+ERR+RES-DFS+OUT+SCH\", \"fyp.CoT-DFS+OUT+SCH-[pass@30]\", \"fyp.CoT-DFS+OUT+SCH-[T]\"]\n",
    "\n",
    "df = df[df[\"experiment_name\"].isin(experiments)]\n",
    "df['experiment_name'] = df['experiment_name'].apply(lambda e: experiment_name_map[e])\n",
    "df['dataset_src'] = df['dataset_src'].apply(lambda e: dict({\"existing_tasks\": \"Existing Tasks\", \"new_tasks\": \"New Tasks\"})[e])\n",
    "df['model'] = df['model'].apply(lambda e: dict({\"LLAMA3_INSTRUCT_70B\": \"Llama 3 70B\", \"LLAMA3_INSTRUCT_8B\": \"Llama 3 8B\"})[e])\n",
    "df['experiment_name'] = pd.Categorical(df['experiment_name'], categories=experiment_name_map.values(), ordered=True)\n",
    "df = df.sort_values('experiment_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot(data, y, ylabel, xlabel=\"Model - Dataset\", x=\"model_dataset\", hue=\"experiment_name\", huelabel=\"Experiment Name\", title=None, palette=\"tab20\"):\n",
    "    plt.figure(figsize=(8.5, 4))\n",
    "    ax = sns.barplot(data=data, x=x, y=y, hue=hue, palette=palette)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    leg = plt.legend(title=huelabel)\n",
    "    leg.get_frame().set_alpha(0)\n",
    "    if data[x].nunique() > 5:\n",
    "        plt.xticks(rotation=90)\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def facetgridplot(data, x, xlabel, y, ylabel, z=\"model_dataset\", zlabel=\"Model - Dataset\", hue=\"experiment_name\", huelabel=\"Experiment Name\", fig=(3, 2), args={}, legend=True, sharex=False, sharey=False):\n",
    "    # plt.figure(figsize=fig)\n",
    "    g = sns.FacetGrid(data, col=z, col_wrap=2, sharex=sharex, sharey=sharey, legend_out=True, height=fig[0], aspect=fig[1], **args)\n",
    "    g.map(sns.barplot, x, y, hue, palette=\"tab20\")\n",
    "    # if data[x].nunique() > 5:\n",
    "    #     g.set_xticklabels(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    if legend:\n",
    "        g.add_legend(title=huelabel)\n",
    "    g.set_axis_labels(xlabel, ylabel)\n",
    "    g.set_titles(col_template=\"{col_name}\")\n",
    "    plt.show()\n",
    "\n",
    "def generate_bin_col(data, bin_col, bin_size):\n",
    "    bin_edges = np.arange(0, data[bin_col].max() + bin_size, bin_size)\n",
    "    data[f\"{bin_col}_bin\"] = pd.cut(data[bin_col], bins=bin_edges, right=False, labels=[f\"{i}-{i+bin_size-1}\" for i in bin_edges[:-1]])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"correct\"] = df[\"predictions\"].apply(lambda preds: any(p[\"accuracy\"] == 1.0 for p in preds))\n",
    "df['model_dataset'] = df['model'].str.split(\" \").str[-1] + ' - ' + df['dataset_src']\n",
    "tasks = df.groupby([\"experiment_name\", \"model_dataset\"])[\"correct\"].mean().reset_index()\n",
    "tasks.columns = [\"experiment_name\", \"model_dataset\", \"pass5\"]\n",
    "# tasks = pd.concat([tasks, get_baseline_results(models=[\"PACHINCO\"])], ignore_index=True)   \n",
    "\n",
    "barplot(\n",
    "    data=tasks,\n",
    "    y=\"pass5\",\n",
    "    ylabel=\"pass@5\",\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"figures/results_pass5.pgf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = df.groupby([\"experiment_name\", \"dataset_src\", \"model\"])[\"correct\"].mean().reset_index()\n",
    "df_table.columns = [\"experiment_name\", \"dataset_src\", \"model\", \"pass5\"]\n",
    "df_table = df_table.pivot(index='experiment_name', columns=['dataset_src', 'model'], values='pass5')\n",
    "print(df_table.to_latex(index=True, header=True))\n",
    "# df_table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"<scratchpad>\\n\\nTo find the average division population for different countries having events involving heavy rain, we need to follow these steps:\\n\\n1. Extract the events involving heavy rain from the 'glc' dataframe.\\n2. Filter the events to only include those with a 'landslide_trigger' of 'Heavy Rain'.\\n3. Extract the 'country_name' and 'admin_division_population' columns from the filtered events.\\n4. Group the data by 'country_name' and calculate the average 'admin_division_population' for each country.\\n\\n</scratchpad>\\n\\n<python>\\nheavy_rain_events = glc[glc.landslide_trigger == 'Heavy Rain']\\navg_division_population = heavy_rain_events.groupby('country_name')['admin_division_population'].mean()\\nprint(avg_division_population)\\n</python>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"task_type\"] = df[\"reference_analysis\"].apply(lambda a: a[\"task_type\"])\n",
    "# tasks_type_results = df.groupby([\"experiment_name\", \"model_dataset\", \"task_type\"])[\"correct\"].mean().reset_index()\n",
    "# tasks_type_results[\"correct\"].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# facetgridplot(\n",
    "#     data=tasks_type_results,\n",
    "#     x=\"task_type\",\n",
    "    \n",
    "#     y=\"correct\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = df.explode(\"predictions\")\n",
    "df_preds[\"correct\"] = df_preds[\"predictions\"].apply(lambda p: p[\"accuracy\"] == 1.0)\n",
    "df_preds['model_dataset'] = df_preds['model'].str.split(\"_\").str[-1] + ' - ' + df_preds['dataset_src']\n",
    "df_preds[\"execution_error\"] = df_preds[\"predictions\"].apply(lambda p: p.get(\"execution_error\", False))\n",
    "df_preds[\"error_type\"] = df_preds[\"predictions\"].apply(lambda p: p[\"error_text\"].split(\":\", 1)[0].strip() if p.get(\"error_text\") else (\"Logical Error\" if p[\"accuracy\"] != 1.0 else None))\n",
    "df_preds[\"runtime_error_type\"] = df_preds[\"predictions\"].apply(lambda p: p[\"error_text\"].split(\":\")[1].strip() if \"RuntimeError\" in p.get(\"error_text\", \"\") else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_errors = [\"RuntimeError\", \"Parse Error\", \"Logical Error\"]\n",
    "error_results = df_preds.groupby([\"experiment_name\", \"model\", \"error_type\"]).size().reset_index(name='counts')\n",
    "error_results = error_results[error_results[\"error_type\"].isin(key_errors)]\n",
    "\n",
    "# facetgridplot(\n",
    "#     data=error_results,\n",
    "#     x=\"error_type\",\n",
    "#     xlabel=\"Error Type\",\n",
    "#     y=\"counts\",\n",
    "#     ylabel=\"Num. Errors\",\n",
    "#     legend=False,\n",
    "#     fig=(4,0.9)\n",
    "# )\n",
    "\n",
    "facetgridplot(\n",
    "    data=error_results,\n",
    "    x=\"error_type\",\n",
    "    xlabel=\"Error Type\",\n",
    "    y=\"counts\",\n",
    "    ylabel=\"Num. Errors\",\n",
    "    z=\"model\",\n",
    "    zlabel=\"Model\",\n",
    "    legend=False,\n",
    "    fig=(3.5,1.5),\n",
    "    sharey=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"figures/results_error_types.pgf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_runtime_errors = [\"AttributeError\", \"IndexError\", \"KeyError\", \"NameError\", \"TypeError\", \"ValueError\"]\n",
    "runtime_error_results = df_preds.groupby([\"experiment_name\", \"model\", \"runtime_error_type\"]).size().reset_index(name='counts')\n",
    "runtime_error_results = runtime_error_results[runtime_error_results[\"runtime_error_type\"].isin(key_runtime_errors)]\n",
    "\n",
    "facetgridplot(\n",
    "    data=runtime_error_results,\n",
    "    x=\"model\",\n",
    "    # x=\"runtime_error_type\",\n",
    "    xlabel=\"Model\",\n",
    "    y=\"counts\",\n",
    "    ylabel=\"Num. Errors\",\n",
    "    args=dict(\n",
    "        ylim=(0, 700)\n",
    "    ),\n",
    "    # fig=(8, 12),\n",
    "    z=\"runtime_error_type\",\n",
    "    legend=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"figures/results_runtime_errors.pgf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_incorrect = df_preds[df_preds[\"correct\"] == False]\n",
    "total_incorrect_counts = total_incorrect.groupby([\"experiment_name\", \"model_dataset\"]).size().reset_index(name='total_counts')\n",
    "logical_errors_results = df_preds[(df_preds[\"correct\"] == False) & (df_preds[\"execution_error\"] == False)]\n",
    "logical_errors_counts = logical_errors_results.groupby([\"experiment_name\", \"model_dataset\"]).size().reset_index(name='logical_counts')\n",
    "merged_counts = logical_errors_counts.merge(total_incorrect_counts, on=[\"experiment_name\", \"model_dataset\"])\n",
    "merged_counts['proportion_logical_errors'] = merged_counts['logical_counts'] / merged_counts['total_counts']\n",
    "logical_errors_results = merged_counts[[\"experiment_name\", \"model_dataset\", \"proportion_logical_errors\"]]\n",
    "\n",
    "barplot(\n",
    "    data=logical_errors_results,\n",
    "    y=\"proportion_logical_errors\",\n",
    "    ylabel=\"Logical Errors\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cluster_count\"] = df[\"clusters\"].apply(lambda x: len(x) if x else 0)\n",
    "cluster_count_results = df.groupby([\"model\", \"experiment_name\",\"cluster_count\"]).size().reset_index(name='counts')\n",
    "cluster_count_results = cluster_count_results[(cluster_count_results[\"cluster_count\"] > 0) & (cluster_count_results[\"cluster_count\"] <= 5)]\n",
    "\n",
    "facetgridplot(\n",
    "    data=cluster_count_results,\n",
    "    x=\"cluster_count\",\n",
    "    xlabel=\"Num. Clusters\",\n",
    "    y=\"counts\",\n",
    "    ylabel=\"Frequency\",\n",
    "    z=\"model\",\n",
    "    zlabel=\"Model\",\n",
    "    legend=False,\n",
    "    fig=(3.5,1.5),\n",
    "    args=dict(\n",
    "        ylim=(0, 720)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"figures/results_num_clusters.pgf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_count = lambda calls: sum([len(v) for v in calls[\"modules\"].values()])\n",
    "df[\"complexity\"] = df[\"reference_analysis\"].apply(call_count)\n",
    "df = generate_bin_col(df, \"complexity\", 5)\n",
    "complexity_results = df.groupby([\"model\", \"experiment_name\", \"complexity_bin\"])[\"correct\"].mean().reset_index()\n",
    "\n",
    "facetgridplot(\n",
    "    data=complexity_results,\n",
    "    x=\"complexity_bin\",\n",
    "    xlabel=\"Num. Pandas Calls\",\n",
    "    y=\"correct\",\n",
    "    ylabel=\"pass@5\",\n",
    "    z=\"model\",\n",
    "    zlabel=\"Model\",\n",
    "    legend=False,\n",
    "    # args=dict(\n",
    "    #     ylim=0.8\n",
    "    # ),\n",
    "    fig=(2.5,1.5),\n",
    "    sharey=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"figures/results_pass5_pandas_calls.pgf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_bin_col(df, \"turn_index\", 6)\n",
    "turn_index_res = df.groupby([\"model\", \"experiment_name\", \"turn_index_bin\"])[\"correct\"].mean().reset_index()\n",
    "\n",
    "facetgridplot(\n",
    "    data=turn_index_res,\n",
    "    x=\"turn_index_bin\",\n",
    "    xlabel=\"Turn Index\",\n",
    "    y=\"correct\",\n",
    "    ylabel=\"pass@5\",\n",
    "    z=\"model\",\n",
    "    zlabel=\"Model\",\n",
    "    legend=False,\n",
    "    sharey=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"figures/results_turn_index.pgf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds[\"predictions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prompt_utils import get_num_tokens\n",
    "\n",
    "# df_preds[\"lines\"] = df_preds[\"predictions\"].apply(lambda s: get_num_tokens(s[\"code\"]))\n",
    "df_preds[\"lines\"] = df_preds[\"predictions\"].apply(lambda s: len(s[\"code\"].split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_preds[\"lines\"] = df_preds[\"predictions\"].apply(lambda s: len([l for l in s[\"code\"].split(\"\\n\") if l and not l.startswith(\"#\")]))\n",
    "\n",
    "line_results = generate_bin_col(df_preds[df_preds[\"lines\"] <= 20], \"lines\", 5)\n",
    "line_results = line_results[line_results[\"correct\"].isin([True])]\n",
    "line_results = line_results.groupby([\"model\", \"experiment_name\", \"lines_bin\"]).count().reset_index()\n",
    "\n",
    "facetgridplot(\n",
    "    data=line_results,\n",
    "    x=\"lines_bin\",\n",
    "    xlabel=\"Num. Tokens\",\n",
    "    y=\"correct\",\n",
    "    ylabel=\"pass@5\",\n",
    "    legend=False,\n",
    "    z=\"model\",\n",
    "    zlabel=\"Model\",\n",
    "    fig=(3.5,1.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds[\"num_lines\"] = df_preds[\"reference\"].apply(lambda s: len(s.split(\"\\n\")))\n",
    "df_preds = generate_bin_col(df_preds, \"num_lines\", 5)\n",
    "lines_results = df_preds.groupby([\"model_dataset\", \"experiment_name\", \"num_lines_bin\"])[\"correct\"].mean().reset_index()\n",
    "\n",
    "facetgridplot(\n",
    "    data=lines_results,\n",
    "    x=\"num_lines_bin\",\n",
    "    xlabel=\"Num. Lines\",\n",
    "    y=\"correct\",\n",
    "    ylabel=\"pass@5\",\n",
    "    legend=False,\n",
    "    fig=(3.5,1.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prompt_utils import get_num_tokens\n",
    "\n",
    "df[\"num_tokens\"] = df[\"prompt_input\"].apply(get_num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = generate_bin_col(df, \"num_tokens\", 1000)\n",
    "# turn_index_res = df.groupby([\"model_dataset\", \"experiment_name\", \"num_tokens_bin\"])[\"correct\"].mean().reset_index()\n",
    "\n",
    "# facetgridplot(\n",
    "#     data=turn_index_res,\n",
    "#     x=\"prompt_length_bin\",\n",
    "#     xlabel=\"Turn Index\",\n",
    "#     y=\"correct\",\n",
    "#     ylabel=\"pass@5\",\n",
    "#     legend=False,\n",
    "# )\n",
    "\n",
    "# df_preds[\"lines\"] = df_preds[\"predictions\"].apply(lambda s: len([l for l in s[\"code\"].split(\"\\n\") if l and not l.startswith(\"#\")]))\n",
    "\n",
    "line_results = generate_bin_col(df[df[\"num_tokens\"] <= 4000], \"num_tokens\", 1000)\n",
    "# line_results = line_results[line_results[\"correct\"].isin([True])]\n",
    "line_results = line_results.groupby([\"model\", \"experiment_name\", \"num_tokens_bin\"])[\"correct\"].mean().reset_index()\n",
    "\n",
    "facetgridplot(\n",
    "    data=line_results,\n",
    "    x=\"num_tokens_bin\",\n",
    "    xlabel=\"Num. Tokens\",\n",
    "    y=\"correct\",\n",
    "    ylabel=\"pass@5\",\n",
    "    legend=False,\n",
    "    z=\"model\",\n",
    "    zlabel=\"Model\",\n",
    "    fig=(3.5,1.5),\n",
    "    sharey=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_rerank(r):\n",
    "    if r[\"clusters\"]:\n",
    "        max_len = max(map(len, r[\"clusters\"].values()))\n",
    "        for idx in list(r[\"clusters\"].values())[::-1]:\n",
    "            cr = 0\n",
    "            for i in idx:\n",
    "                if r[\"predictions\"][i][\"accuracy\"] == 1.0:\n",
    "                    cr += 1\n",
    "                if cr == max_len:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "\n",
    "df[\"rerank_accuracy\"] = df.apply(get_correct_rerank, axis=1)\n",
    "rerank_acc = df[(df[\"correct\"] == True)].groupby([\"model_dataset\", \"experiment_name\"])[\"rerank_accuracy\"].mean().reset_index()\n",
    "# rerank_acc = rerank_acc[(rerank_acc[\"correct\"] == True)]\n",
    "\n",
    "barplot(\n",
    "    data=rerank_acc,\n",
    "    y=\"rerank_accuracy\",\n",
    "    ylabel=\"SC Accuracy\",\n",
    "    hue=\"experiment_name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"figures/results_sc.pgf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_name = \"fyp.CoT-DFS+OUT+SCH\"\n",
    "# exp_pandas_method_results = pandas_method_results[pandas_method_results[\"experiment_name\"] == experiment_name]\n",
    "# exp_pandas_method_results = exp_pandas_method_results.melt(id_vars=[\"experiment_name\", \"model_dataset\"], var_name=\"method\", value_name=\"correct\")\n",
    "\n",
    "# barplot(\n",
    "#     data=exp_pandas_method_results,\n",
    "#     x=\"method\",\n",
    "#     y=\"correct\",\n",
    "#     hue=\"model_dataset\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# def get_used_columns(code, vars, dataframes):\n",
    "#     res = defaultdict(list)\n",
    "#     for v in vars:\n",
    "#         if v in dataframes:\n",
    "#             for col in dataframes[v].columns:\n",
    "#                 if col in code:\n",
    "#                     res[v].append(col)\n",
    "#     return res\n",
    "\n",
    "# print(get_used_columns(ref, ref_dataframes, dfs))\n",
    "# print(get_used_columns(pred, pred_dataframes, dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import cache\n",
    "\n",
    "# @cache\n",
    "# def get_dataframe_schemas(notebook_name, turn_index):\n",
    "#     execution_info = get_execution_state(notebook_name)\n",
    "#     cell_execution_info = execution_info[turn_index]\n",
    "#     dfs = {}\n",
    "#     for c in cell_execution_info[::-1]:\n",
    "#         if \"dataframes\" in c:\n",
    "#             dfs = c[\"dataframes\"]\n",
    "#             break\n",
    "#     df_cols = {k: set(v.columns) if isinstance(v, pd.DataFrame) else set(v.index) for k, v in dfs.items()}\n",
    "#     return df_cols\n",
    "\n",
    "# def get_used_columns(code, vars, df_schemas):\n",
    "#     res = defaultdict(set)\n",
    "#     for v in vars:\n",
    "#         if v in df_schemas:\n",
    "#             for col in df_schemas[v]:\n",
    "#                 if str(col) in code:\n",
    "#                     res[v].add(col)\n",
    "#     return res\n",
    "\n",
    "# def get_used_dfs_and_cols(r):\n",
    "#     code_context = r[\"code_context\"]\n",
    "#     ref, pred = r[\"reference\"], r[\"predictions\"][\"code\"]\n",
    "#     ref_dataframes = set(analyse_code(code_context, ref)[\"used_dataframes\"])\n",
    "#     pred_dataframes = set(analyse_code(r[\"code_context\"], pred)[\"used_dataframes\"])\n",
    "#     dfs = get_dataframe_schemas(r[\"notebook_name\"], r[\"turn_index\"])\n",
    "#     ref_cols = get_used_columns(ref, ref_dataframes, dfs)\n",
    "#     pred_cols = get_used_columns(pred, pred_dataframes, dfs)\n",
    "#     return pd.Series(\n",
    "#         {\n",
    "#             \"used_correct_dataframes\": ref_dataframes == pred_dataframes,\n",
    "#             \"used_correct_dataframe_cols\": ref_cols == pred_cols\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "# df_logical_errors = df_preds[(df_preds[\"correct\"] == False) & (df_preds[\"execution_error\"] == False)]\n",
    "# df_logical_errors[[\"used_correct_dataframes\", \"used_correct_dataframe_cols\"]] = df_logical_errors.apply(get_used_dfs_and_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used_correct_df_results = df_logical_errors.groupby([\"model_dataset\", \"experiment_name\"])[\"used_correct_dataframes\"].mean().reset_index()\n",
    "# used_correct_df_col_results = df_logical_errors.groupby([\"model_dataset\", \"experiment_name\"])[\"used_correct_dataframe_cols\"].mean().reset_index()\n",
    "\n",
    "\n",
    "# barplot(\n",
    "#     data=used_correct_df_results,\n",
    "#     y=\"used_correct_dataframes\"\n",
    "# )\n",
    "\n",
    "# barplot(\n",
    "#     data=used_correct_df_col_results,\n",
    "#     y=\"used_correct_dataframe_cols\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "method_groups = {\n",
    "    \"aggregation\": {\n",
    "        \"methods\": {\"groupby\", \"agg\", \"Grouper\"}\n",
    "    },\n",
    "    \"transformation\": {\n",
    "        \"apply\": {\"apply\", \"applymap\", \"map\"},\n",
    "        \"reshape\": {\"pivot_table\", \"pivot\", \"melt\", \"stack\", \"unstack\", \"transpose\"},\n",
    "        \"bin\": {\"cut\", \"qcut\"},\n",
    "        \"explode\": {\"explode\"},\n",
    "        \"time_series\": {\"shift\", \"pct_change\"},\n",
    "        \"compute\": {\"cumsum\", \"diff\", \"rank\"}\n",
    "    },\n",
    "    \"combination\": {\n",
    "        \"methods\": {\"join\", \"merge\", \"concat\", \"append\"}\n",
    "    },\n",
    "    \"selection\": {\n",
    "        \"indexing\": {\"loc\", \"iloc\", \"between\", \"filter\", \"isin\", \"isna\", \"isnull\", \"notnull\", \"query\", \"where\"},\n",
    "        \"sorting\": {\"sort_values\", \"sort_index\"},\n",
    "        \"subset\": {\"nlargest\", \"nsmallest\", \"head\", \"tail\", \"first\", \"last\"}\n",
    "    },\n",
    "    \"cleaning\": {\n",
    "        \"missing_data\": {\"dropna\", \"fillna\", \"ffill\", \"bfill\"},\n",
    "        \"duplicates\": {\"drop_duplicates\", \"duplicated\"},\n",
    "        \"type_conversion\": {\"astype\", \"to_numeric\", \"to_datetime\", \"to_period\", \"to_frame\", \"to_list\", \"tolist\", \"ravel\"},\n",
    "        \"renaming\": {\"rename\", \"rename_axis\"},\n",
    "        \"structure\": {\"reset_index\", \"set_index\", \"reindex\", \"insert\"}\n",
    "    },\n",
    "    \"strings\": {\n",
    "        \"methods\": {\"str\", \"contains\", \"extract\", \"replace\", \"match\", \"sub\", \"startswith\", \"endswith\", \"split\"}\n",
    "    },\n",
    "    \"computation\": {\n",
    "        \"statistics\": {\"mean\", \"max\", \"min\", \"median\", \"mode\", \"std\", \"var\", \"quantile\", \"describe\"},\n",
    "        \"aggregation\": {\"count\", \"sum\", \"nunique\", \"unique\", \"value_counts\"},\n",
    "        \"correlation\": {\"corr\", \"cov\"},\n",
    "        \"arithmetic\": {\"add\", \"div\", \"divide\", \"clip\", \"round\"}\n",
    "    },\n",
    "    \"datetime\": {\n",
    "        \"conversion\": {\"to_datetime\", \"to_timedelta\"},\n",
    "        \"components\": {\"dt.year\", \"dt.month\", \"dt.day\", \"dt.hour\", \"dt.minute\", \"dt.second\"},\n",
    "        \"operations\": {\"strftime\", \"strptime\", \"tz_localize\", \"tz_convert\"},\n",
    "        \"periods\": {\"to_period\", \"PeriodIndex\", \"period_range\"},\n",
    "        \"timedelta\": {\"Timedelta\", \"timedelta_range\"},\n",
    "        \"offsets\": {\"DateOffset\", \"BDay\", \"CDay\", \"Week\", \"MonthEnd\", \"YearEnd\"}\n",
    "    },\n",
    "    \"visualization\": {\n",
    "        \"methods\": {\"plot\", \"boxplot\", \"hist\", \"lmplot\", \"barplot\", \"scatter\", \"lmplot\", \"barplot\", \"Figure\", \"Layout\", \"Bar\", \"show\", \"Scatter\"}\n",
    "    },\n",
    "}\n",
    "\n",
    "method_groups_flattened = {category: set(itertools.chain.from_iterable(subcategories.values())) \n",
    "                  for category, subcategories in method_groups.items()}\n",
    "\n",
    "method_group_priority = [\n",
    "    \"aggregation\",\n",
    "    \"combination\",\n",
    "    \"transformation\",\n",
    "    \"computation\",\n",
    "    \"selection\",\n",
    "    \"cleaning\",\n",
    "    \"datetime\",\n",
    "    \"strings\",\n",
    "    \"visualization\"\n",
    "]\n",
    "\n",
    "def classify_tasks(row):\n",
    "    result = {f\"is_{category}\": False for category in method_groups_flattened.keys()}\n",
    "    for category, methods in method_groups_flattened.items():\n",
    "        if any(method in row[\"reference\"] for method in methods):\n",
    "            result[f\"is_{category}\"] = True\n",
    "    if any(op in row[\"reference\"] for op in set({\"*\", \"/\", \"+\"})):\n",
    "        result['is_computation'] = True\n",
    "    if any(op in row[\"reference\"] for op in set({\"==\", \">\", \"<\"})):\n",
    "        result['is_selection'] = True\n",
    "    result['task_type'] = \"other\"\n",
    "    for category in method_group_priority:\n",
    "        if result[f\"is_{category}\"]:\n",
    "            result['task_type'] = category\n",
    "            break\n",
    "    return pd.Series(result)\n",
    "\n",
    "df[\"calls\"] = df[\"reference_analysis\"].apply(lambda ms: [ss for s in ms[\"function_calls\"] for ss in s.split(\".\")[1:] if ss])\n",
    "res = df.apply(classify_tasks, axis=1)\n",
    "for col in res.columns:\n",
    "    df[col] = res[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type_res[\"task_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts = df[df[\"task_type\"].isin([\"aggregation\", \"computation\", \"selection\", \"transformation\"])]\n",
    "task_type_res = df_ts.groupby([\"model\", \"experiment_name\", \"task_type\"])[\"correct\"].mean().reset_index()\n",
    "task_type_res[\"task_type\"] = task_type_res[\"task_type\"].apply(lambda s: s.capitalize())\n",
    "\n",
    "facetgridplot(\n",
    "    data=task_type_res,\n",
    "    x=\"model\",\n",
    "    xlabel=\"Model\",\n",
    "    y=\"correct\",\n",
    "    ylabel=\"pass@5\",\n",
    "    z=\"task_type\",\n",
    "    zlabel=\"Task Type\",\n",
    "    legend=False,\n",
    "    sharey=True,\n",
    "     fig=(2.5,2),\n",
    "     args={\n",
    "         \"ylim\":(0,1.0)\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"figures/results_task_type_pass5.pgf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in method_group_priority:\n",
    "    df_res = df[df[f\"is_{g}\"] == True].groupby([\"model_dataset\", \"experiment_name\"])[\"correct\"].mean().reset_index()\n",
    "    barplot(\n",
    "        data=df_res,\n",
    "        y=\"correct\",\n",
    "        ylabel=\"pass@5\",\n",
    "        title=g.capitalize()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multistep_experiments_names = [\"CoT\", \"CoT + 2 Step\", \"[+] Errors\", \"[+] Outputs\"]\n",
    "df_ms = df[df[\"experiment_name\"].isin(multistep_experiments_names)]\n",
    "df_ms[\"correct\"] = df_ms[\"predictions\"].apply(lambda preds: any(p[\"accuracy\"] == 1.0 for p in preds))\n",
    "df_ms['model_dataset'] = df_ms['model'].str.split(\"_\").str[-1] + ' - ' + df['dataset_src']\n",
    "# df_ms = df_ms.explode(\"predictions\")\n",
    "# df_ms[\"correct\"] = df_ms[\"predictions\"].apply(lambda p: p[\"accuracy\"] == 1.0)\n",
    "# df_ms['model_dataset'] = df_ms['model'].str.split(\"_\").str[-1] + ' - ' + df_ms['dataset_src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df_ms.pivot_table(index=['notebook_name', 'turn_index', 'model_dataset'], columns='experiment_name', values='correct', aggfunc=lambda x: list(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_map = {\n",
    "    (False, True):  \"Improved\",\n",
    "    (True, True): \"Still Correct\",\n",
    "    (True, False): \"Degraded\",\n",
    "    (False, False): \"Still Incorrect\"\n",
    "}\n",
    "\n",
    "def get_improvement(r):\n",
    "    original = r[\"CoT\"].iloc[0]\n",
    "    rewrite = r[\"CoT + 2 Step\"].iloc[0]\n",
    "    with_errors = r[\"[+] Errors\"].iloc[0]\n",
    "    with_outputs =  r[\"[+] Outputs\"].iloc[0]\n",
    "    res = []\n",
    "    return pd.Series({\n",
    "        'CoT + 2 Step': [corr_map[(original[i], rewrite[i])] for i in range(len(original))],\n",
    "        '[+] Errors': [corr_map[(original[i], with_errors[i])] for i in range(len(original))],\n",
    "        '[+] Outputs': [corr_map[(original[i], with_outputs[i])] for i in range(len(original))],\n",
    "    })\n",
    "\n",
    "\n",
    "df_improved = df_pivot.groupby(level=['notebook_name', 'turn_index', 'model_dataset']).apply(lambda s: get_improvement(s)).reset_index()\n",
    "df_improved = df_improved.explode(['CoT + 2 Step', '[+] Errors', '[+] Outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df = df_improved.melt(id_vars=['notebook_name', 'turn_index', 'model_dataset'], \n",
    "                    value_vars=['CoT + 2 Step', '[+] Errors', '[+] Outputs'], \n",
    "                    var_name='experiment_name', \n",
    "                    value_name='change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_order = ['CoT + 2 Step', '[+] Errors', '[+] Outputs']\n",
    "\n",
    "# Count the occurrences of each improvement value for each experiment\n",
    "df_counts = melted_df[melted_df[\"change\"].isin([\"Degraded\", \"Improved\"])].groupby(['experiment_name', 'model_dataset', 'change']).size().reset_index(name='count')\n",
    "df_counts[\"count\"] /= 1137\n",
    "\n",
    "\n",
    "facetgridplot(\n",
    "    data=df_counts,\n",
    "    x=\"change\",\n",
    "    xlabel=\"Change Type\",\n",
    "    y=\"count\",\n",
    "    ylabel=\"Frequency\",\n",
    "    # z=\"model_dataset\",\n",
    "    # zlabel=\"Model - Dataset\",\n",
    "    # hue=\"experiment\",\n",
    "    # huelabel=\"Experiment Name\",\n",
    "    args=dict(\n",
    "        ylim=(0, 0.055)\n",
    "    ),\n",
    "    fig=(3,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"figures/results_multistep.pgf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted.groupby(['model_dataset', 'experiment', 'improvement']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in [\"fyp.CoT+2S-DFS+OUT+SCH\", \"fyp.CoT+2S+ERR-DFS+OUT+SCH\", \"fyp.CoT+2S+ERR+RES-DFS+OUT+SCH\"]:\n",
    "    for m in [\"llama3_instruct_8b\", \"llama3_instruct_70b\"]:\n",
    "        fn = f\"/root/FYP/experiments/{ex}/predictions.{m}.json\"\n",
    "        with open(fn) as f:\n",
    "            dataset = json.loads(f.read())\n",
    "\n",
    "        error_pairs = []\n",
    "\n",
    "        for n in dataset:\n",
    "            for t in n[\"turns\"]:\n",
    "                original_evals = t[\"metadata\"][\"example\"][\"metadata\"][\"initial_eval_results\"]\n",
    "                current_evals = t[\"eval_results\"]\n",
    "                for o, c in zip(original_evals, current_evals):\n",
    "                    error_pairs.append((parse_error(o), parse_error(c)))\n",
    "\n",
    "        df = pd.DataFrame(error_pairs, columns=['error1', 'error2'])\n",
    "        contingency_table = pd.crosstab(df['error1'], df['error2'])\n",
    "        mask = np.triu(np.ones_like(contingency_table, dtype=bool))\n",
    "        corr_values = contingency_table.where(~mask)\n",
    "        vmax = corr_values.max().max()\n",
    "\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        sns.heatmap(\n",
    "            contingency_table, \n",
    "            annot=True, \n",
    "            cmap='inferno', \n",
    "            vmin=0,\n",
    "            vmax=vmax*1.6, \n",
    "            center=0.0,\n",
    "            annot_kws={\"size\": 8},\n",
    "            mask=mask,\n",
    "            fmt='d'\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.title(f\"{ex} - {m}\")\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_runtime_errors = [\"AttributeError\", \"IndexError\", \"KeyError\", \"NameError\", \"TypeError\", \"ValueError\"]\n",
    "key_errors = [\"RuntimeError\", \"Parse Error\", \"Logical Error\"]\n",
    "\n",
    "def parse_error(p, split_runtime=False):\n",
    "    if p[\"accuracy\"] != 1.0:\n",
    "        if p.get(\"error_text\"):\n",
    "            error_type = p[\"error_text\"].split(\":\", 1)[0].strip()\n",
    "            if error_type == \"RuntimeError\":\n",
    "                if split_runtime:\n",
    "                    runtime_error = p[\"error_text\"].split(\":\")[1].strip()\n",
    "                    if runtime_error in key_runtime_errors:\n",
    "                        return runtime_error\n",
    "                else:\n",
    "                    return \"Runtime Error\"\n",
    "            elif error_type == \"Parse Error\":\n",
    "                return error_type\n",
    "            return \"Other Error\"\n",
    "        else:\n",
    "            return \"Logical Error\"\n",
    "    return \"Correct\"\n",
    "\n",
    "error_pairs = []\n",
    "\n",
    "for n in dataset:\n",
    "    for t in n[\"turns\"]:\n",
    "        original_evals = t[\"metadata\"][\"example\"][\"metadata\"][\"initial_eval_results\"]\n",
    "        current_evals = t[\"eval_results\"]\n",
    "        for o, c in zip(original_evals, current_evals):\n",
    "            error_pairs.append((parse_error(o), parse_error(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(error_pairs, columns=['error1', 'error2'])\n",
    "contingency_table = pd.crosstab(df['error1'], df['error2'])\n",
    "mask = np.triu(np.ones_like(contingency_table, dtype=bool))\n",
    "corr_values = contingency_table.where(~mask)\n",
    "vmax = corr_values.max().max()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(\n",
    "    contingency_table, \n",
    "    annot=True, \n",
    "    cmap='inferno', \n",
    "    vmin=0,\n",
    "    vmax=vmax*1.6, \n",
    "    center=0.0,\n",
    "    annot_kws={\"size\": 8},\n",
    "    mask=mask,\n",
    "    fmt='d'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.title('Correlation between Pandas Method groups')\n",
    "# plt.savefig('figures/pandas_method_corr.pgf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "experiments = [\"fyp.CoT+2S-DFS+OUT+SCH\", \"fyp.CoT+2S+ERR-DFS+OUT+SCH\", \"fyp.CoT+2S+ERR+RES-DFS+OUT+SCH\"]\n",
    "models = [\"llama3_instruct_8b\", \"llama3_instruct_70b\"]\n",
    "\n",
    "key_runtime_errors = [\"AttributeError\", \"IndexError\", \"KeyError\", \"NameError\", \"TypeError\", \"ValueError\"]\n",
    "key_errors = [\"RuntimeError\", \"Parse Error\", \"Logical Error\"]\n",
    "\n",
    "def parse_error(p, split_runtime=False):\n",
    "    if p[\"accuracy\"] != 1.0:\n",
    "        if p.get(\"error_text\"):\n",
    "            error_type = p[\"error_text\"].split(\":\", 1)[0].strip()\n",
    "            if error_type == \"RuntimeError\":\n",
    "                if split_runtime:\n",
    "                    runtime_error = p[\"error_text\"].split(\":\")[1].strip()\n",
    "                    if runtime_error in key_runtime_errors:\n",
    "                        return runtime_error\n",
    "                else:\n",
    "                    return \"Runtime Error\"\n",
    "            elif error_type == \"Parse Error\":\n",
    "                return error_type\n",
    "            return \"Other Error\"\n",
    "        else:\n",
    "            return \"Logical Error\"\n",
    "    return \"Correct\"\n",
    "\n",
    "exmap = {\n",
    "    \"fyp.CoT+2S-DFS+OUT+SCH\": \"CoT + 2 Step\",\n",
    "    \"fyp.CoT+2S+ERR-DFS+OUT+SCH\": \"[+] Errors\",\n",
    "    \"fyp.CoT+2S+ERR+RES-DFS+OUT+SCH\": \"[+] Outputs\",\n",
    "}\n",
    "\n",
    "mmap = {\n",
    "    \"llama3_instruct_8b\": \"Llama 3 8B\",\n",
    "    \"llama3_instruct_70b\": \"Llama 3 70B\",\n",
    "}\n",
    "\n",
    "# Prepare data for FacetGrid\n",
    "plot_data = []\n",
    "\n",
    "for ex in experiments:\n",
    "    for m in models:\n",
    "        fn = f\"/root/FYP/experiments/{ex}/predictions.{m}.json\"\n",
    "        with open(fn) as f:\n",
    "            dataset = json.loads(f.read())\n",
    "\n",
    "        error_pairs_ns = []\n",
    "        error_pairs_es = []\n",
    "\n",
    "        for n in dataset:\n",
    "            for t in n[\"turns\"]:\n",
    "                original_evals = t[\"metadata\"][\"example\"][\"metadata\"][\"initial_eval_results\"]\n",
    "                current_evals = t[\"eval_results\"]\n",
    "                for o, c in zip(original_evals, current_evals):\n",
    "                    oe = parse_error(o)\n",
    "                    ce = parse_error(c)\n",
    "                    if \"Other Error\" not in [oe, ce]:   \n",
    "                        print(n[\"metadata\"][\"dataset_src\"])\n",
    "                        if n[\"metadata\"][\"dataset_src\"] == \"new_tasks\":\n",
    "                            error_pairs_ns.append((parse_error(o), parse_error(c)))\n",
    "                        else:\n",
    "                            error_pairs_es.append((parse_error(o), parse_error(c)))\n",
    "\n",
    "        for n, ds in [(\"New Tasks\", error_pairs_ns), (\"Existing Tasks\", error_pairs_es)]:\n",
    "            df = pd.DataFrame(ds, columns=['error1', 'error2'])\n",
    "            contingency_table = pd.crosstab(df['error1'], df['error2']).stack().reset_index(name='count')\n",
    "            contingency_table['experiment'] = exmap[ex]\n",
    "            contingency_table['model'] = mmap[m] + \" - \" + n\n",
    "            plot_data.append(contingency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data into a single DataFrame\n",
    "plot_df = pd.concat(plot_data, ignore_index=True)\n",
    "\n",
    "error_order = [\"Correct\", \"Logical Error\", \"Parse Error\", \"Runtime Error\"]\n",
    "plot_df['error1'] = pd.Categorical(plot_df['error1'], categories=error_order, ordered=True)\n",
    "plot_df['error2'] = pd.Categorical(plot_df['error2'], categories=error_order, ordered=True)\n",
    "\n",
    "\n",
    "# Create the FacetGrid\n",
    "g = sns.FacetGrid(plot_df, col='experiment', row='model', margin_titles=True)\n",
    "\n",
    "g.fig.set_size_inches(11, 11)\n",
    "\n",
    "# Map the heatmap to the FacetGrid\n",
    "def plot_heatmap(data, **kwargs):\n",
    "    data = data.pivot('error1', 'error2', 'count')\n",
    "    data = data.div(data.sum(axis=0), axis=1)\n",
    "    mask = np.triu(np.ones_like(data, dtype=bool))\n",
    "    vmax = data.max().max()\n",
    "    sns.heatmap(\n",
    "        data, \n",
    "        annot=True, \n",
    "        cmap='inferno', \n",
    "        vmin=0,\n",
    "        vmax=1, \n",
    "        center=0.0,\n",
    "        annot_kws={\"size\": 8},\n",
    "        # mask=mask,\n",
    "        fmt='.0%',\n",
    "        cbar=False,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "g.map_dataframe(plot_heatmap)\n",
    "g.set_axis_labels('Original', 'Updated')\n",
    "g.set_titles(col_template='{col_name}', row_template='{row_name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"figures/results_multistep_errors.pgf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = aggregate_results()\n",
    "df_t = df[df[\"experiment_name\"] == \"fyp.CoT-DFS+OUT+SCH-[T]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temps = df_t.explode(\"predictions\")\n",
    "df_temps['temperature'] = df_temps.groupby(level=0).cumcount().apply(lambda i: round((i % 6) * 0.2, 1))\n",
    "df_temps[\"correct\"] = df_temps[\"predictions\"].apply(lambda p: p[\"accuracy\"] == 1.0)\n",
    "df_temps['model_dataset'] = df_temps['model'].str.split(\"_\").str[-1] + ' - ' + df_temps['dataset_src']\n",
    "temp_results = df_temps.groupby([\"model_dataset\", \"temperature\"])[\"correct\"].mean().reset_index()\n",
    "temp_results[\"correct\"] += 0.1\n",
    "# temp_avg_results = df_temps.groupby([\"model\", \"temperature\"]).agg(pass1=('correct', np.mean)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(\n",
    "    data=temp_results,\n",
    "    y=\"correct\",\n",
    "    ylabel=\"pass@5\",\n",
    "    hue=\"temperature\",\n",
    "    huelabel=\"Temperature\",\n",
    "    palette=\"viridis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"\"\"\n",
    "<scratchpad>\n",
    "Step 1: Extract the relevant columns from the cars dataframe.\n",
    "Step 2: Group the dataframe by 'Make' and 'Year' to get the unique models for each manufacturer and year.\n",
    "Step 3: Use the groupby object to create a new dataframe with the manufacturers as the index and years as columns.\n",
    "Step 4: Reset the index to create a multi-indexed dataframe with the manufacturers as the index and years as columns.\n",
    "</scratchpad>\n",
    "\n",
    "<python>\n",
    "cars_model_year = cars.groupby(['Make', 'Year'])['Model'].nunique().unstack().reset_index()\n",
    "cars_model_year.columns = ['Make', 'Year', 'Models']\n",
    "print(cars_model_year)\n",
    "\"\"\"\n",
    "\n",
    "from src.prompt_utils import extract_code_from_response\n",
    "\n",
    "a, b = extract_code_from_response(abc)\n",
    "\n",
    "print(a)\n",
    "print(\"*****\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"/root/FYP/experiments/fyp.CoT-FS+DFS+OUT+SCH/predictions.llama3_instruct_8b.json\"\n",
    "\n",
    "\n",
    "with open(fname) as f:\n",
    "    ds = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(fname, \"w\") as f:\n",
    "#     f.write(json.dumps(ds, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "for n in ds:\n",
    "    for t in n[\"turns\"]:\n",
    "        del t[\"eval_results\"]\n",
    "        for i in range(5):\n",
    "            # print(t[\"predictions\"][i])\n",
    "            # print(\"**********\")\n",
    "            # print(t[\"original\"][i])\n",
    "            # print(\"**********\")\n",
    "            print(t[\"predictions\"][i])\n",
    "            if extract_code_from_response(t[\"original\"][i])[0] == t[\"predictions\"][i]:\n",
    "                x += 1\n",
    "            # print(extract_code_from_response(t[\"original\"][i])[0])\n",
    "        t[\"predictions\"] = [extract_code_from_response(o)[0] for o in t[\"original\"]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
