{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/root/FYP\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils import get_baseline_results\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS_ROOT = \"experiments\"\n",
    "\n",
    "def aggregate_results():\n",
    "    results = []\n",
    "    for rootdir, dirs, files in os.walk(EXPERIMENTS_ROOT):\n",
    "        for subdir in dirs:\n",
    "            expdir = os.path.join(rootdir, subdir)\n",
    "            results_path = os.path.join(expdir, \"results.json\")\n",
    "            if os.path.exists(results_path):\n",
    "                with open(results_path, \"r\") as f:\n",
    "                    data = json.loads(f.read())\n",
    "                results.extend(list(map(lambda e: e | {\"experiment_name\": str(subdir)}, data)))\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "experiment_name_map = {\n",
    "    \"fyp.DFS+OUT+FS+SCH\": \"[+] Schema Information\",\n",
    "    \"fyp.CoT-DFS+OUT+SCH\": \"CoT\",\n",
    "    \"fyp.CoT+2S-DFS+OUT+SCH\": \"CoT + 2 Step\",\n",
    "    \"fyp.CoT+2S+ERR-DFS+OUT+SCH\": \"[+] Errors\",\n",
    "}\n",
    "\n",
    "df = aggregate_results()\n",
    "\n",
    "def extract_single_temp(r):\n",
    "    t = 0.6\n",
    "    i = int(t / 0.2)\n",
    "    n = 5\n",
    "    if r[\"experiment_name\"] == \"vanilla_raw_notebook\":\n",
    "        return r[\"predictions\"][i*n:(i+1)*n] \n",
    "    return r[\"predictions\"]\n",
    "\n",
    "df[\"predictions\"] = df.apply(extract_single_temp, axis=1)\n",
    "\n",
    "experiments = [\"fyp.DFS+OUT+FS+SCH\", \"fyp.CoT-DFS+OUT+SCH\", \"fyp.CoT+2S-DFS+OUT+SCH\", \"fyp.CoT+2S+ERR-DFS+OUT+SCH\"]\n",
    "df = df[df[\"experiment_name\"].isin(experiments)]\n",
    "df['experiment_name'] = df['experiment_name'].apply(lambda e: experiment_name_map[e])\n",
    "df['dataset_src'] = df['dataset_src'].apply(lambda e: dict({\"existing_tasks\": \"Existing Tasks\", \"new_tasks\": \"New Tasks\"})[e])\n",
    "\n",
    "experiment_name_map = {\n",
    "    \"fyp.DFS+OUT+FS+SCH\": \"[+] Schema Information\",\n",
    "    \"fyp.CoT-DFS+OUT+SCH\": \"CoT\",\n",
    "    \"fyp.CoT+2S-DFS+OUT+SCH\": \"CoT + 2 Step\",\n",
    "    \"fyp.CoT+2S+ERR-DFS+OUT+SCH\": \"[+] Errors\",\n",
    "}\n",
    "df['experiment_name'] = pd.Categorical(df['experiment_name'], categories=experiment_name_map.values(), ordered=True)\n",
    "df = df.sort_values('experiment_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# fname = \"/root/FYP/experiments/fyp.CoT-FS+DFS+OUT+SCH/results.json\"\n",
    "\n",
    "# with open(fname, \"r\") as f:\n",
    "#     df = pd.DataFrame(json.loads(f.read()))\n",
    "\n",
    "df_preds = df.explode(\"predictions\")\n",
    "df_preds[\"correct\"] = df_preds[\"predictions\"].apply(lambda p: p[\"accuracy\"] == 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from codebleu import calc_codebleu\n",
    "from pprint import pprint\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "def generate_metrics(predictions, references):\n",
    "    results = {}\n",
    "    results[\"bleu\"] = bleu.compute(predictions=predictions, references=references)[\"bleu\"]\n",
    "    results |= meteor.compute(predictions=predictions, references=references)\n",
    "    results |= rouge.compute(predictions=predictions, references=references)\n",
    "    results[\"chrf\"] = chrf.compute(predictions=predictions, references=references)[\"score\"]\n",
    "    results |= calc_codebleu(predictions, references, lang=\"python\")\n",
    "    return results\n",
    "\n",
    "references = df_preds[\"reference\"].values[:10]\n",
    "predictions = df_preds[\"predictions\"].apply(lambda p: p[\"code\"]).values[:10]\n",
    "pprint(generate_metrics(predictions, references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(df):\n",
    "    references = df[\"reference\"].values\n",
    "    predictions = df[\"predictions\"].apply(lambda p: p[\"code\"]).values\n",
    "    metrics =  generate_metrics(predictions, references)\n",
    "    return pd.Series(metrics)\n",
    "\n",
    "\n",
    "df_metrics = df_preds.groupby([\"experiment_name\", \"model\", \"dataset_src\", \"correct\"]).apply(calc_metrics).reset_index()\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics[[c for c in df_metrics.columns if not c.endswith(\"match_score\")]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name_map = {\n",
    "    \"fyp.DFS+OUT+FS+SCH\": \"[+] Schema Information\",\n",
    "    \"fyp.CoT-DFS+OUT+SCH\": \"CoT\",\n",
    "    \"fyp.CoT+2S-DFS+OUT+SCH\": \"CoT + 2 Step\",\n",
    "    \"fyp.CoT+2S+ERR-DFS+OUT+SCH\": \"[+] Errors\",\n",
    "}\n",
    "melted_df['experiment_name'] = pd.Categorical(melted_df['experiment_name'], categories=experiment_name_map.values(), ordered=True)\n",
    "melted_df = melted_df.sort_values('experiment_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['experiment_name'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_metrics[[\"experiment_name\", \"model\", \"dataset_src\", \"correct\", \"bleu\", \"meteor\", \"chrf\", \"rougeL\"]]\n",
    "df_res = df_res[df_res[\"experiment_name\"] == \"CoT\"]\n",
    "df_res['model_dataset'] = df_res['model'].str.split(\"_\").str[-1] + ' - ' + df_res['dataset_src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[\"chrf\"] /= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df[\"experiment_name\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming the data is already in a DataFrame called 'df'\n",
    "# If not, you can create it like this:\n",
    "# df = pd.read_csv('your_data.csv')  # or whatever format your data is in\n",
    "\n",
    "# Melt the DataFrame to make it easier to plot\n",
    "melted_df = pd.melt(df_res, id_vars=['model', 'dataset_src', 'correct'], \n",
    "                    value_vars=['bleu', 'meteor', 'chrf', 'rougeL'],\n",
    "                    var_name='metric', value_name='score')\n",
    "\n",
    "melted_df[\"model\"] = melted_df[\"model\"].apply(lambda m: \"Llama 3 70B\" if \"70B\" in m else \"Llama 3 8B\")\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Define color palettes\n",
    "palette_70B = sns.color_palette(\"Greens_d\", 2)\n",
    "palette_8B = sns.color_palette(\"Reds_d\", 2)\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "# Create the grouped bar plot\n",
    "g = sns.catplot(x='metric', y='score', hue='model', col='dataset_src',\n",
    "                data=melted_df, kind='bar', height=4, aspect=1.2,\n",
    "                palette=sns.color_palette(\"Greens_d\", 2),\n",
    "                legend_out=False, sharex=False)\n",
    "\n",
    "# Customize the plot\n",
    "g.set_axis_labels(\"Results\", \"Score\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.legend.remove()\n",
    "# g.add_legend(title=\"Correct\")\n",
    "# g.fig.suptitle(\"Model Performance Comparison\", fontsize=16)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "# plt.tight_layout()\n",
    "# plt.subplots_adjust(top=0.93)\n",
    "plt.legend(title=\"Model\", loc='upper right',bbox_to_anchor=(1,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"figures/results_nlp_metrics.pgf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_res.groupby('model_dataset').apply(compute_corr)\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_res is your dataframe\n",
    "# df_res = df_metrics[[\"model_dataset\", \"correct\", \"bleu\", \"meteor\", \"chrf\", \"rougeL\"]]\n",
    "\n",
    "# Create a function to compute correlation matrix\n",
    "def compute_corr(data):\n",
    "    return data.corr()\n",
    "\n",
    "# Group by model_dataset and compute correlation\n",
    "grouped = df_res.groupby('model_dataset').apply(compute_corr)\n",
    "grouped = grouped.reset_index()\n",
    "grouped = grouped.melt(id_vars=['model_dataset', 'level_1'], \n",
    "                       var_name='level_2', \n",
    "                       value_name='correlation')\n",
    "\n",
    "# Create FacetGrid\n",
    "g = sns.FacetGrid(grouped, col='model_dataset', col_wrap=2, height=4, aspect=1)\n",
    "\n",
    "# Define heatmap function\n",
    "def plot_heatmap(data, **kwargs):\n",
    "    data = data.pivot('level_1', 'level_2', 'correlation')\n",
    "    mask = np.triu(np.ones_like(data, dtype=bool))\n",
    "    sns.heatmap(\n",
    "        data, \n",
    "        annot=True, \n",
    "        cmap='coolwarm', \n",
    "        # vmin=-1,\n",
    "        # vmax=1, \n",
    "        # center=0,\n",
    "        annot_kws={\"size\": 8},\n",
    "        # mask=mask,\n",
    "        fmt='.2f',\n",
    "        cbar=False,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "# Map the heatmap to the FacetGrid\n",
    "g.map_dataframe(plot_heatmap)\n",
    "\n",
    "# Set titles and labels\n",
    "g.set_titles('{col_name}')\n",
    "g.set_axis_labels('', '')\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_corr = df_metrics[[\"bleu\", \"meteor\", \"chrf\", \"rougeL\", \"correct\"]].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_corr, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds[\"predictions\"].apply(lambda p: p[\"recall\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
