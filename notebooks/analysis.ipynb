{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/root/FYP\")\n",
    "import json\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from src.analysis import analyse_code\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.use(\"pgf\")\n",
    "# matplotlib.rcParams.update({\n",
    "#     \"pgf.texsystem\": \"pdflatex\",\n",
    "#     'font.family': 'serif',\n",
    "#     'text.usetex': True,\n",
    "#     'pgf.rcfonts': False,\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_ROOT = \"datasets\"\n",
    "\n",
    "def generate_tasks_df():\n",
    "    with open(os.path.join(DATASETS_ROOT, \"dataset.formatted.json\")) as f:\n",
    "        notebooks = json.loads(f.read())\n",
    "    with open(os.path.join(\"resources\", \"task_return_types.json\")) as f:\n",
    "        return_types = json.loads(f.read())\n",
    "    tasks = []\n",
    "    for notebook in notebooks:\n",
    "        for i, turn in enumerate(notebook[\"turns\"]):\n",
    "            task = {\n",
    "                \"dataset_src\": notebook[\"dataset_src\"],\n",
    "                \"notebook_name\": notebook[\"notebook_name\"],\n",
    "                \"turn_index\": i,\n",
    "                \"code_context\": turn[\"input\"],\n",
    "                \"intent\": turn[\"turn\"][\"intent\"][\"value\"],\n",
    "                \"code\": turn[\"turn\"][\"code\"][\"value\"],\n",
    "                \"return_types\": return_types[str((notebook[\"dataset_src\"], notebook[\"notebook_name\"], i))]\n",
    "            }\n",
    "            task |= analyse_code(task[\"code_context\"], task[\"code\"])\n",
    "            task[\"task_type\"] = task[\"task_type\"].split(\".\")[-1]\n",
    "            task[\"dataset_src\"] = dict({\"existing_tasks\": \"Existing Tasks\", \"new_tasks\": \"New Tasks\"})[task[\"dataset_src\"]]\n",
    "            tasks.append(task)\n",
    "    tasks_path = os.path.join(\"resources\", \"metadata.json\")\n",
    "    df = pd.DataFrame(tasks)\n",
    "    df.to_json(tasks_path)\n",
    "    return df\n",
    "\n",
    "df = generate_tasks_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot(data, y, x, hue=None):\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    fig.set_size_inches(w=5.90666, h=4.5)\n",
    "    sns.barplot(data=data, x=x, y=y, hue=hue, palette=\"tab10\", ax=ax)\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "    plt.title(f'{x} - {y}')\n",
    "    plt.legend(title=hue)\n",
    "    if data[x].nunique() > 3:\n",
    "        plt.xticks(rotation=90)\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.show()\n",
    "\n",
    "def facetgridplot(data, x, y, z, hue):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    g = sns.FacetGrid(data, col=z, col_wrap=2)\n",
    "    g.map(sns.barplot, x, y, hue, palette=\"tab10\")\n",
    "    if data[x].nunique() > 3:\n",
    "        g.set_xticklabels(rotation=90)\n",
    "    g.add_legend(title=hue)\n",
    "    g.set_axis_labels(x, y)\n",
    "    g.set_titles(col_template=\"{col_name}\")\n",
    "    plt.show()\n",
    "\n",
    "def generate_bin_col(data, bin_col, bin_size):\n",
    "    bin_edges = np.arange(0, data[bin_col].max() + bin_size, bin_size)\n",
    "    data[f\"{bin_col}_bin\"] = pd.cut(data[bin_col], bins=bin_edges, right=False, labels=[f\"{i}-{i+bin_size-1}\" for i in bin_edges[:-1]])\n",
    "    return data\n",
    "\n",
    "def histplot(data, xlabel, ylabel=\"Frequency Density\", cntlabel=\"size\", bin_edges=None):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    data = data.copy()\n",
    "    colors = {\n",
    "        'Existing Tasks': 'blue',\n",
    "        'New Tasks': 'green',\n",
    "    }\n",
    "\n",
    "    if bin_edges is None:\n",
    "        bin_edges = np.arange(min(data[cntlabel])-0.5, max(data[cntlabel])+1.5, 1)\n",
    "\n",
    "    for dataset_src in data['dataset_src'].unique():\n",
    "        subset = data[data['dataset_src'] == dataset_src]\n",
    "        \n",
    "        # Calculate density\n",
    "        density, _ = np.histogram(subset[cntlabel], bins=bin_edges, density=True)\n",
    "        \n",
    "        # Plot density histogram\n",
    "        plt.bar(bin_edges[:-1], density, width=np.diff(bin_edges), \n",
    "                alpha=0.5, edgecolor=\"k\", label=dataset_src, \n",
    "                color=colors.get(dataset_src), align='edge')\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(title='Dataset Source')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTotal Tasks: \", len(df))\n",
    "print(\"\\nTotal Notebooks: \", len(df.groupby(\"notebook_name\")))\n",
    "print(\"\\nTasks per\", df.groupby(\"dataset_src\").size())\n",
    "print(\"\\nNotebooks per\", df.groupby([\"dataset_src\",\"notebook_name\"]).size().groupby(level=0).size())\n",
    "\n",
    "\n",
    "dataset_summary = pd.DataFrame({\n",
    "    'Tasks': df.groupby(\"dataset_src\").size(),\n",
    "    'Notebooks': df.groupby([\"dataset_src\", \"notebook_name\"]).size().groupby(level=0).size()\n",
    "})\n",
    "\n",
    "# Reset the index to make 'dataset_src' a column\n",
    "dataset_summary = dataset_summary.reset_index()\n",
    "\n",
    "dataset_summary['Tasks per Notebook'] = dataset_summary['Tasks'] / dataset_summary['Notebooks']\n",
    "\n",
    "# Rename the 'index' column to 'Dataset'\n",
    "dataset_summary = dataset_summary.rename(columns={'dataset_src': 'Dataset'})\n",
    "dataset_summary['Tasks per Notebook'] = dataset_summary['Tasks per Notebook'].round(2)\n",
    "\n",
    "# Display the dataset summary\n",
    "print(dataset_summary.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_count = df.groupby([\"dataset_src\", \"notebook_name\"]).size().reset_index(name='size')\n",
    "\n",
    "histplot(\n",
    "    data=task_count,\n",
    "    xlabel='Num. Notebook Problems',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('figures/num_tasks.pgf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_count = lambda calls: sum([len(v) for v in calls.values()])\n",
    "df[\"size\"] = df[\"modules\"].apply(call_count)\n",
    "histplot(\n",
    "    data=df,\n",
    "    xlabel=\"Num. Pandas API Calls\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('figures/num_calls.pgf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_imports = lambda s: {i.split(\".\")[0] for i in set.union(*s)}\n",
    "df_notebook_imports = df.groupby(\"notebook_name\")[\"imports\"].agg(agg_imports)\n",
    "import_count = Counter(itertools.chain(*map(list, df_notebook_imports.values)))\n",
    "to_show = 10\n",
    "sorted_values = sorted(import_count.values(), reverse=True)[:to_show]\n",
    "sorted_keys = sorted(import_count, key=import_count.get, reverse=True)[:to_show]\n",
    "df_res = pd.DataFrame({\"imports\": sorted_keys, \"freq\": sorted_values})\n",
    "\n",
    "barplot(\n",
    "    data=df_res,\n",
    "    x=\"imports\",\n",
    "    y=\"freq\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis import PANDAS_METHODS\n",
    "\n",
    "pandas_calls = Counter(itertools.chain(*map(lambda m: m[\"pandas\"], df[\"modules\"].values)))\n",
    "pandas_calls = {k: v for k, v in pandas_calls.items() if k in PANDAS_METHODS}\n",
    "\n",
    "to_display = 20\n",
    "sorted_values = sorted(pandas_calls.values(), reverse=True)[:to_display]\n",
    "sorted_keys = sorted(pandas_calls, key=pandas_calls.get, reverse=True)[:to_display]\n",
    "df_res = pd.DataFrame({\"calls\": sorted_keys, \"freq\": sorted_values})\n",
    "\n",
    "barplot(\n",
    "    data=df_res,\n",
    "    x=\"calls\",\n",
    "    y=\"freq\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_return_types = Counter(itertools.chain(*df[\"return_types\"].values))\n",
    "sorted_values = sorted(task_return_types.values(), reverse=True)\n",
    "sorted_keys = sorted(task_return_types, key=task_return_types.get, reverse=True)\n",
    "df_res = pd.DataFrame({\"return_types\": sorted_keys, \"freq\": sorted_values})\n",
    "df_res[\"percentage\"] = (df_res[\"freq\"] / df_res[\"freq\"].sum()) * 100\n",
    "\n",
    "barplot(\n",
    "    data=df_res,\n",
    "    x=\"return_types\",\n",
    "    y=\"freq\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prompt_utils import get_num_tokens\n",
    "\n",
    "df[\"num_tokens\"] = df[\"code_context\"].apply(get_num_tokens)\n",
    "\n",
    "histplot(\n",
    "    data=df,\n",
    "    xlabel=\"Num. Tokens\",\n",
    "    cntlabel=\"num_tokens\",\n",
    "    bin_edges = np.arange(0, 4000, 200)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ratio_md_code(r):\n",
    "    cc = r[\"code_context\"]\n",
    "    md = \"\\n\".join([ss for s in cc.split(\"# In[ ]:\") for ss in s.split(\"\\n\") if ss.startswith(\"#\")])\n",
    "    code = \"\\n\".join([ss for s in cc.split(\"# In[ ]:\") for ss in s.split(\"\\n\") if ss.strip() and not ss.startswith(\"#\")])\n",
    "    e = 0.000000000000000000000000000000001\n",
    "    return len(md)\n",
    "    # return (r[\"size\"] +e)/(len(md) +e)\n",
    "\n",
    "df[\"md_code_ratio\"] = df.apply(calc_ratio_md_code, axis=1)\n",
    "\n",
    "histplot(\n",
    "    data=df,\n",
    "    xlabel=\"Markdown\",\n",
    "    cntlabel=\"md_code_ratio\",\n",
    "    bin_edges = np.arange(0, 3000, 50)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prompt_utils import get_num_tokens\n",
    "\n",
    "df[\"num_lines\"] = df[\"code_context\"].apply(lambda s: len([ss for ss in s.split(\"\\n\") if ss.strip() and not ss.strip().startswith(\"#\")]))\n",
    "\n",
    "histplot(\n",
    "    data=df,\n",
    "    xlabel=\"Num. Tokens\",\n",
    "    cntlabel=\"num_lines\",\n",
    "    bin_edges = np.arange(0, 200, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dataframes_tot\"] = df[\"dataframes\"].apply(lambda l: len(set(l)))\n",
    "\n",
    "histplot(\n",
    "    data=df,\n",
    "    xlabel=\"Num. Dataframes/Series\",\n",
    "    cntlabel=\"dataframes_tot\",\n",
    "    bin_edges = np.arange(0, 30, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('figures/num_dataframes.pgf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.savefig('figures/num_tokens.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_res[[\"return_types\", \"percentage\"]].round(2).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/FYP/experiments/arcade.CoT-FS+EXP/predictions.llama3_instruct_70b.json\") as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "from src.multistep import parse_outputs\n",
    "\n",
    "data = {}\n",
    "\n",
    "return_type_map = {\n",
    "    'int': \"Numeric\",\n",
    "    'float': \"Numeric\",\n",
    "    'str': \"String\",\n",
    "    'NoneType': \"Empty\",\n",
    "    'list': \"List/Tuple/Set\",\n",
    "    'bool': \"Boolean\",\n",
    "    'tuple': \"List/Tuple/Set\",\n",
    "    'set': \"List/Tuple/Set\"\n",
    "}\n",
    "\n",
    "def get_output_type(v):\n",
    "    parsed_output = parse_outputs(v)\n",
    "    if isinstance(parsed_output, str) :\n",
    "        if \"dtype\" in parsed_output:\n",
    "            return \"Series\"\n",
    "        elif \"Figure\" in parsed_output:\n",
    "            return \"Plot\"\n",
    "    t = type(parsed_output).__name__\n",
    "    return return_type_map.get(t, t)\n",
    "\n",
    "for n in dataset:\n",
    "    for i, t in enumerate(n[\"turns\"]):\n",
    "        data[str((n[\"metadata\"][\"dataset_src\"], n[\"metadata\"][\"notebook_name\"], i))] = [get_output_type(v) for v in t.get(\"ref_output_html\", {}).values()]\n",
    "\n",
    "with open(\"resources/task_return_types.json\", \"w\") as f:\n",
    "    f.write(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_groups = {\n",
    "    \"aggregation\": {\n",
    "        \"methods\": {\"groupby\", \"agg\", \"Grouper\"}\n",
    "    },\n",
    "    \"transformation\": {\n",
    "        \"apply\": {\"apply\", \"applymap\", \"map\"},\n",
    "        \"reshape\": {\"pivot_table\", \"pivot\", \"melt\", \"stack\", \"unstack\", \"transpose\"},\n",
    "        \"bin\": {\"cut\", \"qcut\"},\n",
    "        \"explode\": {\"explode\"},\n",
    "        \"time_series\": {\"shift\", \"pct_change\"},\n",
    "        \"compute\": {\"cumsum\", \"diff\", \"rank\"}\n",
    "    },\n",
    "    \"combination\": {\n",
    "        \"methods\": {\"join\", \"merge\", \"concat\", \"append\"}\n",
    "    },\n",
    "    \"selection\": {\n",
    "        \"indexing\": {\"loc\", \"iloc\", \"between\", \"filter\", \"isin\", \"isna\", \"isnull\", \"notnull\", \"query\", \"where\"},\n",
    "        \"sorting\": {\"sort_values\", \"sort_index\"},\n",
    "        \"subset\": {\"nlargest\", \"nsmallest\", \"head\", \"tail\", \"first\", \"last\"}\n",
    "    },\n",
    "    \"cleaning\": {\n",
    "        \"missing_data\": {\"dropna\", \"fillna\", \"ffill\", \"bfill\"},\n",
    "        \"duplicates\": {\"drop_duplicates\", \"duplicated\"},\n",
    "        \"type_conversion\": {\"astype\", \"to_numeric\", \"to_datetime\", \"to_period\", \"to_frame\", \"to_list\", \"tolist\", \"ravel\"},\n",
    "        \"renaming\": {\"rename\", \"rename_axis\"},\n",
    "        \"structure\": {\"reset_index\", \"set_index\", \"reindex\", \"insert\"}\n",
    "    },\n",
    "    \"strings\": {\n",
    "        \"methods\": {\"str\", \"contains\", \"extract\", \"replace\", \"match\", \"sub\", \"startswith\", \"endswith\", \"split\"}\n",
    "    },\n",
    "    \"computation\": {\n",
    "        \"statistics\": {\"mean\", \"max\", \"min\", \"median\", \"mode\", \"std\", \"var\", \"quantile\", \"describe\"},\n",
    "        \"aggregation\": {\"count\", \"sum\", \"nunique\", \"unique\", \"value_counts\"},\n",
    "        \"correlation\": {\"corr\", \"cov\"},\n",
    "        \"arithmetic\": {\"add\", \"div\", \"divide\", \"clip\", \"round\"}\n",
    "    },\n",
    "    \"datetime\": {\n",
    "        \"conversion\": {\"to_datetime\", \"to_timedelta\"},\n",
    "        \"components\": {\"dt.year\", \"dt.month\", \"dt.day\", \"dt.hour\", \"dt.minute\", \"dt.second\"},\n",
    "        \"operations\": {\"strftime\", \"strptime\", \"tz_localize\", \"tz_convert\"},\n",
    "        \"periods\": {\"to_period\", \"PeriodIndex\", \"period_range\"},\n",
    "        \"timedelta\": {\"Timedelta\", \"timedelta_range\"},\n",
    "        \"offsets\": {\"DateOffset\", \"BDay\", \"CDay\", \"Week\", \"MonthEnd\", \"YearEnd\"}\n",
    "    },\n",
    "    \"visualization\": {\n",
    "        \"methods\": {\"plot\", \"boxplot\", \"hist\", \"lmplot\", \"barplot\", \"scatter\", \"lmplot\", \"barplot\", \"Figure\", \"Layout\", \"Bar\", \"show\", \"Scatter\"}\n",
    "    },\n",
    "}\n",
    "\n",
    "method_groups_flattened = {category: set(itertools.chain.from_iterable(subcategories.values())) \n",
    "                  for category, subcategories in method_groups.items()}\n",
    "\n",
    "method_group_priority = [\n",
    "    \"aggregation\",\n",
    "    \"combination\",\n",
    "    \"transformation\",\n",
    "    \"computation\",\n",
    "    \"selection\",\n",
    "    \"cleaning\",\n",
    "    \"datetime\",\n",
    "    \"strings\",\n",
    "    \"visualization\"\n",
    "]\n",
    "\n",
    "def classify_tasks(row):\n",
    "    result = {f\"is_{category}\": False for category in method_groups_flattened.keys()}\n",
    "    for category, methods in method_groups_flattened.items():\n",
    "        if any(method in row[\"code\"] for method in methods):\n",
    "            result[f\"is_{category}\"] = True\n",
    "    if any(op in row[\"code\"] for op in set({\"*\", \"/\", \"+\"})):\n",
    "        result['is_computation'] = True\n",
    "    if any(op in row[\"code\"] for op in set({\"==\", \">\", \"<\"})):\n",
    "        result['is_selection'] = True\n",
    "    result['task_type'] = \"other\"\n",
    "    for category in method_group_priority:\n",
    "        if result[f\"is_{category}\"]:\n",
    "            result['task_type'] = category\n",
    "            break\n",
    "    return pd.Series(result)\n",
    "\n",
    "df[\"calls\"] = df[\"function_calls\"].apply(lambda ms: [ss for s in ms for ss in s.split(\".\")[1:] if ss])\n",
    "res = df.apply(classify_tasks, axis=1)\n",
    "for col in res.columns:\n",
    "    df[col] = res[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(list(method_groups_flattened.items()), columns=['Group', 'Methods'])\n",
    "# df[\"Group\"] = df[\"Group\"].apply(lambda s: s.capitalize())\n",
    "# df[\"Methods\"] = df[\"Methods\"].apply(lambda l: f\"{', '.join(list(l)[:5])} {', ...' if len(l) > 5 else ''}\")\n",
    "# print(df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_columns = [col for col in df.columns if col.startswith('is_')]\n",
    "percentage_true = df[is_columns].mean() * 100\n",
    "result_df = pd.DataFrame({\n",
    "    'Task Type': percentage_true.index,\n",
    "    '%': percentage_true.values\n",
    "}).round(1)\n",
    "result_df[\"Task Type\"] = result_df[\"Task Type\"].apply(lambda s: s.split(\"_\")[-1].capitalize())\n",
    "print(result_df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "is_columns = [col for col in df.columns if col.startswith('is_')]\n",
    "correlation_matrix = df[is_columns].corr().round(2)\n",
    "rename_func = lambda x: x.split(\"_\")[-1].capitalize()\n",
    "correlation_matrix = correlation_matrix.rename(columns=rename_func, index=rename_func)\n",
    "\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "corr_values = correlation_matrix.where(~mask)\n",
    "vmax = corr_values.max().max()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    annot=True, \n",
    "    cmap='inferno', \n",
    "    vmin=-0.3,\n",
    "    vmax=0.3, \n",
    "    center=0.0,\n",
    "    annot_kws={\"size\": 8},\n",
    "    mask=mask\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.title('Correlation between Pandas Method groups')\n",
    "plt.savefig('figures/pandas_method_corr.pgf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_columns = [col for col in df.columns if col.startswith('is_')]\n",
    "percentage_true = df[is_columns].mean() * 100\n",
    "result_df = pd.DataFrame({\n",
    "    'Task Type': percentage_true.index,\n",
    "    '%': percentage_true.values\n",
    "}).round(1)\n",
    "result_df[\"Task Type\"] = result_df[\"Task Type\"].apply(lambda s: s.split(\"_\")[-1].capitalize())\n",
    "print(result_df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prompts import select_dataframes_by_priority\n",
    "\n",
    "code = \"\"\"\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('NYC_Restaurants.csv', dtype=str)\n",
    "\n",
    "df_noduplicates = df.drop_duplicates(subset='RESTAURANT')\n",
    "df_notchains = df_noduplicates.groupby(\"DBA\").filter(lambda x: len(x) == 1)\n",
    "boro_notchain_pivot = pd.pivot_table(df_notchains, index = 'BORO', values = 'RESTAURANT', aggfunc = lambda x: len(x.unique()))\n",
    "boro_restaurant_pivot = pd.pivot_table(df_noduplicates, index = 'BORO', values = 'RESTAURANT', aggfunc = lambda x: len(x.unique()))\n",
    "boro_notchain_pivot['TOTAL RESTAURANTS'] = boro_restaurant_pivot\n",
    "\n",
    "cuis = df_noduplicates['CUISINE DESCRIPTION'].value_counts()\n",
    "mask = (df['VIOLATION CODE']).isnull()\n",
    "no_violations = df[mask]\n",
    "no_violations[['CUISINE DESCRIPTION', 'RESTAURANT']]\n",
    "\n",
    "cuisine_no_violations = no_violations['CUISINE DESCRIPTION'].value_counts()\n",
    "mask = (df['VIOLATION CODE']).notnull()\n",
    "violations = df[mask]\n",
    "cuisine_violations = violations['CUISINE DESCRIPTION'].value_counts()\n",
    "\n",
    "total_cuisine = pd.concat([cuisine_violations , cuisine_no_violations], axis = 1)\n",
    "violations = pd.crosstab(df['BORO'], df['VIOLATION DESCRIPTION']).query('BORO != [\"Missing\"]')\n",
    "\n",
    "vstack = violations.stack()\n",
    "violations2 = vstack.unstack('BORO')\n",
    "mostcommon = DataFrame({'Most Common Complaint':violations2.idxmax(),'Number of Complaints':violations2.max()}) \n",
    "\"\"\"\n",
    "\n",
    "select_dataframes_by_priority(code, max_dataframes=12, display=True)\n",
    "plt.savefig('figures/dataframe_dep_graph.pgf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "<notebook_context>\\n[showing 24 out of 30 notebook cells]\\n\\n# In[]:\\nprint(df_airing)\\n\\n# Out[]:\\n[showing 1 sample rows out of 66 rows]\\n                       name (str) studio (str)                theme (str) tags (str) source (str)  rating (float64)  year (float64)                                          synopsis (str) demographic (str) status (str)  eps (float64)  eps_avg_duration_in_min (float64) rated_by (str)  numeric_ratings (float64)\\n1955  'Ninja Bear the Animation\\u2605'     'KOO-KI'  'Anthropomorphic, School'   'Comedy'   'Original'               NaN          2017.0  'The Ninja International ...icSchool\\\\nDemographicKids'            'Kids'     'Airing'            NaN                                0.0          '401'                      401.0\\n\\n# In[]:\\nprint(df_anim)\\n\\n# Out[]:\\n[showing 3 sample rows out of 1812 rows]\\n                                              theme (str)                 name (str)  rating (float64)\\n124   [' Historical', ' Parody', ' Samurai', 'Gag Humor']                 'Gintama''              9.05\\n1303                                           ['School']  'Kareshi Kanojo no Jijou'              7.60\\n1488                                     ['Mahou Shoujo']  'Delicious Party\\u2661Precure'              7.24\\n\\n# In[]:\\nprint(df_dem)\\n\\n# Out[]:\\n[showing 3 sample rows out of 458 rows]\\n                             numeric_ratings (float64)\\ndemographic studio                                    \\nKids        A-1 Pictures                         839.0\\nShounen     AXsiZ                             183000.0\\n            Kyoto Animation                   779000.0\\n\\n# In[]:\\nprint(df_dur)\\n\\n# Out[]:\\n[showing 3 sample rows out of 2402 rows]\\n     source (str)  eps (float64)  eps_avg_duration_in_min (float64)  average_total_duration (float64)\\n212       'Manga'          161.0                               24.0                            3864.0\\n1877      'Manga'           48.0                                9.0                             432.0\\n2342      'Other'           26.0                               24.0                             624.0\\n\\n# In[]:\\nprint(df_tag)\\n\\n# Out[]:\\n[showing 3 sample rows out of 8759 rows]\\n        theme (str)   tags (str)\\n192   'Super Power'  'Adventure'\\n1398       'School'      'Drama'\\n1556      ' Shoujo'      ' Life'\\n\\n# In[]:\\n# What is the most common tag associated with each theme?\\n\\n# In[]:\\ndf_tag = df[['theme', 'tags']].dropna()\\ndf_tag.theme = df_tag.theme.apply(split_fn)\\ndf_tag.tags = df_tag.tags.apply(split_fn)\\ndf_tag = df_tag.explode(column=['theme']).explode('tags')\\ndf_tag.groupby('theme').agg(pd.Series.mode)\\n\\n# Out[]:\\n[showing 3 sample rows out of 101 rows]\\n              tags (ndarray)\\ntheme                       \\n Historical           Action\\nMedical               Comedy\\nReverse Harem        Romance\\n\\n# In[]:\\nprint(df_tag)\\n\\n# Out[]:\\n[showing 3 sample rows out of 8759 rows]\\n        theme (str)   tags (str)\\n192   'Super Power'  'Adventure'\\n1398       'School'      'Drama'\\n1556      ' Shoujo'      ' Life'\\n\\n# In[]:\\nprint(df_rating)\\n\\n# Out[]:\\n[showing 3 sample rows out of 1898 rows]\\n                                     name (str)  numeric_ratings (float64)  rating (float64) theme (str)\\n52    'Nanatsu no Taizai: Imashime no Fukkatsu'                   986000.0              7.64   'Shounen'\\n1103                                 'Bakuon!!'                    58000.0              6.48     'CGDCT'\\n1331                          'Kyou kara Maou!'                   103000.0              7.65    'Isekai'\\n\\n# In[]:\\n# List the average total duration of anime series based on their source.\\n\\n# In[]:\\ndf_dur = df[['source', 'eps', 'eps_avg_duration_in_min']].dropna()\\ndf_dur['average_total_duration'] = df.eps * df.eps_avg_duration_in_min\\ndf_dur[['source', 'average_total_duration']].groupby('source').mean()\\n\\n# Out[]:\\n[showing 3 sample rows out of 15 rows]\\n             average_total_duration (float64)\\nsource                                       \\nLight novel                            498.30\\nMusic                                   90.00\\nOther                                  697.98\\n\\n# In[]:\\nprint(df_dur)\\n\\n# Out[]:\\n[showing 3 sample rows out of 2402 rows]\\n     source (str)  eps (float64)  eps_avg_duration_in_min (float64)  average_total_duration (float64)\\n212       'Manga'          161.0                               24.0                            3864.0\\n1877      'Manga'           48.0                                9.0                             432.0\\n2342      'Other'           26.0                               24.0                             624.0\\n\\n# In[]:\\nprint(df_studio)\\n\\n# Out[]:\\n[showing 3 sample rows out of 242 rows]\\n     studio (str)  rating (float64)\\n459   'Bee Train'              7.04\\n1500    'Unknown'              5.77\\n1728      'dwarf'              6.30\\n\\n# In[]:\\n# What are the most popular studios in terms of number of ratings for each demographic?\\n\\n# In[]:\\ndf_dem = df[['studio', 'demographic', 'numeric_ratings']]\\ndf_dem = df_dem.groupby(['demographic', 'studio']).sum().sort_values('numeric_ratings', ascending=False)\\ndf_dem.reset_index().groupby('demographic').first()\\n\\n# Out[]:\\n               studio (str)  numeric_ratings (float64)\\ndemographic                                           \\nJosei            'Madhouse'                  1032000.0\\nKids                  'OLM'                  1680859.0\\nSeinen           'Madhouse'                 10278900.0\\nShoujo       'Brain's Base'                  3296000.0\\nShounen             'Bones'                 20615200.0\\n\\n# In[]:\\nprint(df_theme)\\n\\n# Out[]:\\n[showing 3 sample rows out of 3896 rows]\\n          theme (str)  rated_by (float64)\\n330   ' Martial Arts'            105000.0\\n2105           'Kids'               267.0\\n2414           'Kids'                73.0\\n\\n# In[]:\\nprint(df_dem)\\n\\n# Out[]:\\n[showing 3 sample rows out of 458 rows]\\n                             numeric_ratings (float64)\\ndemographic studio                                    \\nKids        A-1 Pictures                         839.0\\nShounen     AXsiZ                             183000.0\\n            Kyoto Animation                   779000.0\\n\\n# In[]:\\n# List the highest rated anime for each theme.\\n\\n# In[]:\\ndf_anim = df[['theme', 'name', 'rating']].dropna()\\ndf_anim.theme = df_anim.theme.apply(split_fn)\\ndf_anim.sort_values(by='rating', ascending=False).explode('theme').groupby('theme').first()\\n\\n# Out[]:\\n[showing 3 sample rows out of 100 rows]\\n                                                  name (str)  rating (float64)\\ntheme                                                                         \\n High Stakes Game  'Gyakkyou Burai Kaiji: Ultimate Survivor'              8.28\\nMedical                                    'Kuuchuu Buranko'              7.94\\nReverse Harem                'Yamato Nadeshiko Shichihenge\\u2665'              7.73\\n\\n# In[]:\\nprint(df_anim)\\n\\n# Out[]:\\n[showing 3 sample rows out of 1812 rows]\\n                                              theme (str)                 name (str)  rating (float64)\\n124   [' Historical', ' Parody', ' Samurai', 'Gag Humor']                 'Gintama''              9.05\\n1303                                           ['School']  'Kareshi Kanojo no Jijou'              7.60\\n1488                                     ['Mahou Shoujo']  'Delicious Party\\u2661Precure'              7.24\\n\\n# In[]:\\n# What are the top 3 highest rated anime still on air?\\n\\n# In[]:\\nprint(df)\\n\\n# Out[]:\\n[showing 1 sample rows out of 3005 rows]\\n               name (str) studio (str)  theme (str)                                 tags (str) source (str)  rating (float64)  year (float64)                                          synopsis (str) demographic (str) status (str)  eps (float64)  eps_avg_duration_in_min (float64) rated_by (str)\\n226  'Ushio to Tora (TV)'      'MAPPA'  'Mythology'  'Action, Adventure, Comedy, Supernatural'      'Manga'              7.58          2015.0  'Ushio Aotsuki is a stubb...ology\\\\nDemographicShounen'         'Shounen'   'Finished'           26.0                               24.0         '207K'\\n\\n# In[]:\\n\\ndf_airing = df[df.status == 'Airing']\\ndf_airing[['name', 'rating']].sort_values(by='rating', ascending=False).head(3)\\n\\n# Out[]:\\n                                        name (str)  rating (float64)\\n119                                 'Spy x Family'              9.09\\n927   'Kaguya-sama wa Kokurasetai: Ultra Romantic'              8.97\\n1167                          'Kingdom 4th Season'              8.69\\n\\n# In[]:\\nprint(df_airing)\\n\\n# Out[]:\\n[showing 1 sample rows out of 66 rows]\\n                       name (str) studio (str)                theme (str) tags (str) source (str)  rating (float64)  year (float64)                                          synopsis (str) demographic (str) status (str)  eps (float64)  eps_avg_duration_in_min (float64) rated_by (str)  numeric_ratings (float64)\\n1955  'Ninja Bear the Animation\\u2605'     'KOO-KI'  'Anthropomorphic, School'   'Comedy'   'Original'               NaN          2017.0  'The Ninja International ...icSchool\\\\nDemographicKids'            'Kids'     'Airing'            NaN                                0.0          '401'                      401.0\\n</notebook_context>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# Columns in df_anim with example values:\\n# Id (addam-marbrand), Label (Addam Marbrand), Allegiances (Lannister), Gender (1.0), Nobility (1.0), GoT (1.0), CoK (1.0), SoS (1.0), FfC (1.0), DwD (0.0), Dead (0.0)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_context = \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create sample data\n",
    "dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'sales': np.random.randint(100, 1000, size=len(dates)),\n",
    "    'customer_id': np.random.randint(1, 101, size=len(dates)),\n",
    "    'product_category': np.random.choice(['A', 'B', 'C', 'D'], size=len(dates))\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "code = \"\"\"\n",
    "# Define a custom function\n",
    "def calculate_category_metrics(group):\n",
    "    return pd.Series({\n",
    "        'total_sales': group['sales'].sum(),\n",
    "        'avg_sales': group['sales'].mean(),\n",
    "        'max_sales': group['sales'].max(),\n",
    "        'unique_customers': group['customer_id'].nunique(),\n",
    "        'sales_per_customer': group['sales'].sum() / group['customer_id'].nunique()\n",
    "    })\n",
    "\n",
    "# Apply the custom function\n",
    "monthly_metrics = df.set_index('date').groupby([pd.Grouper(freq='M'), 'product_category']).apply(calculate_category_metrics).reset_index()\n",
    "\n",
    "# Further data manipulation\n",
    "monthly_metrics['month_year'] = monthly_metrics['date'].dt.strftime('%B %Y')\n",
    "result = monthly_metrics.sort_values(['date', 'product_category']).reset_index(drop=True)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis import analyse_code\n",
    "\n",
    "analyse_code(code_context, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
