{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import types\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "os.chdir(\"/root/FYP\")\n",
    "from src.prompt_utils import format_dataframe_str, string_format\n",
    "\n",
    "from src.utils import from_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSE_PROMPT = \"\"\"\n",
    "Output an analysis of your answer in the <analysis> tag.\n",
    "Explain the underlying reasoning that the question is asking for.\n",
    "Explain whether this is truly reflected in your answer.\n",
    "Take a deep breath and work on this problem step-by-step.\n",
    "If you believe your answer is correct, do not make unnecessary changes.\n",
    "If you believe your answer is incorrect, explain what needs to be changed to ensure correctness.\n",
    "Output the final code in the <python> tag.\n",
    "\"\"\"\n",
    "\n",
    "ANALYSE_ERR_PROMPT = \"\"\"\n",
    "Your solution was executed in the stateful Jupyter notebook environment.\n",
    "An error occurred during execution of the code you submitted: \n",
    "<error>\n",
    "{error_text}\n",
    "</error>\n",
    "\n",
    "Output an analysis of your answer and the error in the <analysis> tag.\n",
    "Explain the underlying reasoning that the question is asking for.\n",
    "Explain whether this is truly reflected in your answer.\n",
    "Take a deep breath and work on this problem step-by-step.\n",
    "Explain what needs to be changed to ensure correctness.\n",
    "Output the final code in the <python> tag.\n",
    "\"\"\"\n",
    "\n",
    "ANALYSE_ERR_OUTPUT_PROMPT = \"\"\"\n",
    "Your solution was executed in the stateful Jupyter notebook environment.\n",
    "Here are the execution results of the code you submitted:\n",
    "<results>\n",
    "{output}\n",
    "</results>\n",
    "\n",
    "Output an analysis of your answer in the <analysis> tag.\n",
    "Explain the underlying reasoning that the question is asking for.\n",
    "Explain whether this is truly reflected in your answer and results.\n",
    "Take a deep breath and work on this problem step-by-step.\n",
    "If you believe your answer is correct, do not make unnecessary changes.\n",
    "If you believe your answer is incorrect, explain what needs to be changed to ensure correctness.\n",
    "Output the final code in the <python> tag.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables(html_string):\n",
    "    try:\n",
    "        return pd.read_html(io.StringIO(html_string))[0]\n",
    "    except ValueError:\n",
    "        match = re.search(r'<th>(.*?)</th>', html_string)\n",
    "        if match:\n",
    "            column_name = match.group(1)\n",
    "            return pd.DataFrame(columns=[column_name])\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "def parse_outputs(v):\n",
    "    if not v:\n",
    "        return v\n",
    "    if \"<code>\" in v:\n",
    "        out = re.findall(r'<code>(.*?)<\\/code>', v, re.DOTALL)[0]\n",
    "        try:\n",
    "            out = ast.literal_eval(out)\n",
    "        except (SyntaxError, ValueError):\n",
    "            pass\n",
    "        return out\n",
    "    elif \"table\" in v:\n",
    "        return extract_tables(v)\n",
    "\n",
    "def format_var(v):\n",
    "    if isinstance(v, pd.DataFrame):\n",
    "        v = format_dataframe_str(v, max_rows=3)\n",
    "    elif isinstance(v, str) and \"Name:\" in v:\n",
    "        lines = v.strip().split(\"\\n\")\n",
    "        if len(lines) > 7:\n",
    "            v = \"\\n\".join(lines[:3] + [\"...\"] + lines[-3:])\n",
    "    else:\n",
    "        v = str(string_format(v))\n",
    "    return v\n",
    "\n",
    "def format_vars(output_html):\n",
    "    output_vars = {k: parse_outputs(v) for k, v in output_html.items()}\n",
    "    prompt = []\n",
    "    for k, v in output_vars.items():\n",
    "        if not isinstance(v, types.NoneType):\n",
    "            if k == \"__output__\":\n",
    "                prompt.append(\"OUTPUT:\")\n",
    "                prompt.append(format_var(v))\n",
    "            else:\n",
    "                prompt.append(f\"NAME: {k}\")\n",
    "                prompt.append(\"VALUE:\")\n",
    "                prompt.append(format_var(v))\n",
    "            prompt.append(\"\\n\")\n",
    "    return \"\\n\".join(prompt).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analysis_prompt(output, eval_results):\n",
    "    return [{\"role\": \"assistant\", \"content\": output}, {\"role\": \"user\", \"content\": ANALYSE_PROMPT}]\n",
    "\n",
    "def generate_analysis_err_prompt(output, eval_results):\n",
    "    msg = ANALYSE_PROMPT\n",
    "    if eval_results.get(\"error_text\"):\n",
    "        msg = ANALYSE_ERR_PROMPT.format(error_text=eval_results[\"error_text\"])     \n",
    "    return [{\"role\": \"assistant\", \"content\": output}, {\"role\": \"user\", \"content\": msg}]\n",
    "\n",
    "def generate_analysis_err_out_prompt(output, eval_results):\n",
    "    msg = ANALYSE_PROMPT\n",
    "    if eval_results.get(\"error_text\"):\n",
    "        msg = ANALYSE_ERR_PROMPT.format(error_text=eval_results[\"error_text\"])\n",
    "    elif eval_results.get(\"output_html\"):\n",
    "        msg = ANALYSE_ERR_OUTPUT_PROMPT.format(output=format_vars(eval_results[\"output_html\"]))\n",
    "    return [{\"role\": \"assistant\", \"content\": output}, {\"role\": \"user\", \"content\": msg}]\n",
    "\n",
    "def generate_dervived_dataset(experiment_name, base_experiment_name, response_fn):\n",
    "    dataset = json.loads(open(os.path.join(\"datasets\", f\"dataset.{base_experiment_name}.json\")).read())\n",
    "    config = json.loads(open(os.path.join(\"experiments\", base_experiment_name, f\"config.json\")).read())\n",
    "    for m in config[\"models\"]:\n",
    "        m = m.lower()\n",
    "        evaluation = json.loads(open(os.path.join(\"experiments\", base_experiment_name, f\"predictions.{m}.json\")).read())\n",
    "        for ni, n in enumerate(dataset):\n",
    "            for ti, t in enumerate(n[\"turns\"]):\n",
    "                outputs = evaluation[ni][\"turns\"][ti][\"original\"]\n",
    "                eval_results = evaluation[ni][\"turns\"][ti][\"eval_results\"]\n",
    "                t[\"metadata\"][\"initial_eval_results\"] = eval_results\n",
    "                t[\"user_messages\"] = [response_fn(o, e) for o, e in zip(outputs, eval_results)]\n",
    "        dataset_path = os.path.join(\"datasets\", f\"dataset.{experiment_name}.{m}.json\")\n",
    "        with open(dataset_path, \"w\") as f:\n",
    "            f.write(json.dumps(dataset, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dervived_dataset(\"fyp.CoT+2S-DFS+OUT+SCH\", \"fyp.CoT-DFS+OUT+SCH\", generate_analysis_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFINE_PROMPT = \"\"\"\n",
    "Output an analysis of your answer in the <analysis> tag.\n",
    "Explain the underlying reasoning that the question is asking for.\n",
    "Explain whether this is truly reflected in your answer.\n",
    "Take a deep breath and work on this problem step-by-step.\n",
    "If you believe your answer is correct, do not make unnecessary changes.\n",
    "If you believe your answer is incorrect, explain what needs to be changed to ensure correctness.\n",
    "Output the final code in the <python> tag.\n",
    "\"\"\"\n",
    "\n",
    "def generate_response(output, eval_results):\n",
    "    return [{\"role\": \"assistant\", \"content\": output}, {\"role\": \"user\", \"content\": REFINE_PROMPT}]\n",
    "\n",
    "\n",
    "def build_multistep_dataset(experiment_name, response_fn):\n",
    "    evaluation = json.loads(open(os.path.join(\"experiments\", experiment_name, \"predictions.llama3_instruct_70b.json\")).read())\n",
    "    dataset = json.loads(open(os.path.join(\"datasets\", f\"dataset.{experiment_name}.json\")).read())\n",
    "    for ni, n in enumerate(dataset):\n",
    "        for ti, t in enumerate(n[\"turns\"]):\n",
    "            input_msgs = t[\"input_msgs\"]\n",
    "            outputs = evaluation[ni][\"turns\"][ti][\"original\"]\n",
    "            eval_results = evaluation[ni][\"turns\"][ti][\"eval_results\"]\n",
    "            t[\"user_responses\"] = [response_fn(o, e) for o, e in zip(outputs, eval_results)]\n",
    "            print(input_msgs)\n",
    "            print(outputs[0])\n",
    "            print(eval_results[0])\n",
    "            print(t[\"user_responses\"][0])\n",
    "            break\n",
    "        break \n",
    "\n",
    "build_multistep_dataset(\"fyp.with_dataframes\", generate_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prompt_utils import format_dataframe_str, string_format\n",
    "import re\n",
    "import pandas as pd\n",
    "import io\n",
    "import ast\n",
    "\n",
    "x = 0\n",
    "\n",
    "ERROR_PROMPT = \"\"\"\n",
    "Your solution was executed in the stateful Jupyter notebook environment.\n",
    "An error occurred during execution of the code you submitted: \n",
    "<error_text>\n",
    "{error_text}\n",
    "</error_text>\n",
    "\n",
    "Refine your code and rewrite it for clarity and correctness.\n",
    "Please use the <scratchpad> tag to explain the error.\n",
    "Output the correct code in the <python> tags.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "REFINE_PROMPT = \"\"\"\n",
    "Clean up, refine and rewrite your solution.\n",
    "Think carefully about the question and your steps.\n",
    "Use the <scratchpad> tag for steps.\n",
    "Output code in the <python> tags.\n",
    "\"\"\"\n",
    "\n",
    "REFINE_PROMPT_OUT = \"\"\"\n",
    "Your solution was executed in the stateful Jupyter notebook environment.\n",
    "Here is the outputs of the code you submitted:\n",
    "<outputs>\n",
    "{output}\n",
    "</outputs>\n",
    "\n",
    "Clean up, refine and rewrite your solution.\n",
    "Think carefully about the question and your steps.\n",
    "Use the <scratchpad> tag for steps.\n",
    "Output code in the <python> tags.\n",
    "\n",
    "Think carefully about what the question is asking for and the variables you are working with.\n",
    "Make sure your logical reasoning steps are correct and reflect what the question really means.\n",
    "Refine your steps and simplify your code for clarity, efficiency and correctness.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "RW, RW_EXEC_ERR, RW_EXEC_ERR_OUT\n",
    "\n",
    "# REWRITE - *, NO EXEC\n",
    "# REWRITE ERRS - RUNTIME ERRS, EXEC\n",
    "# REWRITE ERRS + OUTPUTS - *, EXEC\n",
    "\n",
    "# REFINE_PROMPT = \n",
    "\n",
    "from src.prompt_utils import get_num_tokens\n",
    "import types\n",
    "\n",
    "for n in dataset:\n",
    "    for t in n[\"turns\"]:\n",
    "        error_texts = [p.get(\"error_text\") for p in t[\"eval_results\"]]\n",
    "        output_vars = [{k: parse_outputs(v) for k, v in vars.get(\"output_html\").items()} for vars in t[\"eval_results\"] if vars.get(\"output_html\")]\n",
    "        outputs = zip(error_texts, output_vars)\n",
    "\n",
    "        \n",
    "\n",
    "                \n",
    "        # print(ERROR_PROMPT.format(error_text=err_text[0]))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
