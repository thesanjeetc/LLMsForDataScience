{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "notebook = {\n",
    "    \"notebook_name\": \"dataset_student_data/notebook_0/annotated.ipynb\",\n",
    "    \"annotator\": \"annotator_3\",\n",
    "    \"work_dir\": \"dataset_student_data/notebook_0\",\n",
    "    \"provenance\": {\n",
    "      \"guid\": \"76710e9\",\n",
    "      \"url\": \"https://github.com/jf4rr3ll/IS362_Project2/blob/master/Project2.ipynb\",\n",
    "      \"github_repo\": \"https://github.com/jf4rr3ll/IS362_Project2\",\n",
    "      \"sha\": None,\n",
    "      \"artifacts\": {\n",
    "        \"student_data.csv\": {\n",
    "          \"url\": \"https://github.com/jf4rr3ll/IS362_Project2/raw/master/student_data.csv\",\n",
    "          \"md5_checksum\": \"289f2c894c62961940afc702a009b1e4\"\n",
    "        },\n",
    "        \"nyc_death_data.csv\": {\n",
    "          \"url\": \"https://github.com/jf4rr3ll/IS362_Project2/raw/master/nyc_death_data.csv\",\n",
    "          \"md5_checksum\": \"a900deece16be19e6e329c3cace32cf4\"\n",
    "        },\n",
    "        \"california_cities_data.csv\": {\n",
    "          \"url\": \"https://github.com/jf4rr3ll/IS362_Project2/raw/master/california_cities_data.csv\",\n",
    "          \"md5_checksum\": \"ecbd04614e86989473ac35341fb8bb99\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "}\n",
    "task = {\n",
    "            \"input\": \"# In[ ]:\\n\\n\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n\\n# In[ ]:\\n\\n\\n# You are a professional data scientist. Answer the following questions using pandas and matplotlib.\\n\\n\\n# In[ ]:\\n\\n\\n# # Exercise 1\\n\\n\\n# In[ ]:\\n\\n\\ndf = pd.read_csv('employee.csv')\\n\\n\\n# In[ ]:\\n\\n\\n# Schema of Dataframes:\\n# Columns in df with example values:\\n# name (Peter), gender (m), DOB (1992/01/17)\\n\\n\\n# In[ ]:\\n\\n\\n# Problem: How many male and female employees are born in 1992?\\n\\n\\n# In[ ]:\\n\\n\\n# Solution:\\ndf['DOB'] = pd.to_datetime(df['DOB'])\\nnum_male_students = len(df[(df['DOB'].dt.year == 1992) & (df['gender'] == 'm')])\\nnum_female_students = len(df[(df['DOB'].dt.year == 1992) & (df['gender'] == 'f')])\\n\\n\\n# In[ ]:\\n\\n\\n# # Exercise 2\\n\\n\\n# In[ ]:\\n\\n\\ndf = pd.read_csv('scores.csv')\\n\\n\\n# In[ ]:\\n\\n\\n# Schema of Dataframes:\\n# Columns in df with example values:\\n# Stu_Name (Mike), Engineering (90), English (89), Math (92)\\n\\n\\n# In[ ]:\\n\\n\\n# Problem: Get the students with an averaged score above 90 for science subjects.\\n\\n\\n# In[ ]:\\n\\n\\n# Solution:\\ndf['Science_Avg'] = (df['Engineering'] + df['Math']) / 2\\ndf_score_above_90 = df[df['Science_Avg'] > 90]\\nresult = df_score_above_90[['Stu_Name', 'Science_Avg']]\\n\\n\\n# In[ ]:\\n\\n\\n# # Exercise 3\\n\\n\\n# In[ ]:\\n\\n\\ndf = pd.read_csv('geo.csv')\\n\\n\\n# In[ ]:\\n\\n\\n# Schema of Dataframes:\\n# Columns in df with example values:\\n# state (WA), capital (Seattle), population (1.4 millon)\\n\\n\\n# In[ ]:\\n\\n\\n# Problem: What is the population of California?\\n\\n\\n# In[ ]:\\n\\n\\n# Solution:\\nresult = df[df['state'] == 'CA']['population']\\n\\n\\n# In[ ]:\\n\\n\\n# # Exercise 4\\n\\n\\n# In[ ]:\\n\\n\\ndf = pd.read_csv('phones.csv')\\n\\n\\n# In[ ]:\\n\\n\\n# Schema of Dataframes:\\n# Columns in df with example values:\\n# model (Pixel 6), brand (Google), price (387), release (2022)\\n\\n\\n# In[ ]:\\n\\n\\n# Problem: What is the most expensive phone in each brand.\\n\\n\\n# In[ ]:\\n\\n\\n# Solution:\\nmodel_by_brand_df = df.groupby('brand')\\nidx = model_by_brand_df['price'].idxmax()\\nexpensive_models_df = df.loc[idx]\\nresult = expensive_models_df[['brand', 'model', 'price']]\\n\\n\\n# In[ ]:\\n\\n\\n# # Exercise 5\\n\\n\\n# In[ ]:\\n\\n\\n# Schema of Dataframes:\\n# Columns in calidata with example values:\\n# Unnamed: 0 (0), city (Adelanto), latd (34.57611111), longd (-117.4327778), elevation_m (875.0), elevation_ft (2871.0), population_total (31765), area_total_sq_mi (56.027), area_land_sq_mi (56.009), area_water_sq_mi (0.018), area_total_km2 (145.107), area_land_km2 (145.062), area_water_km2 (0.046), area_water_percent (0.03)\\n# Columns in deathdata with example values:\\n# Year (2014), Leading Cause (Diabetes Mellitus (E10-E14)), Sex (F), Race Ethnicity (Other Race/ Ethnicity), Deaths (11), Death Rate (.), Age Adjusted Death Rate (.)\\n\\n\\n# In[ ]:\\n\\n\\ndeathdata = deathdata.replace('.', np.nan)\\n\\n\\n# In[ ]:\\n\\n\\ndeathdata.dtypes\\n\\n\\n# In[ ]:\\n\\n\\n# Converting object data types to numeric\\n\\n\\n# In[ ]:\\n\\n\\ndeathdata[['Deaths']] = deathdata[['Deaths']].apply(pd.to_numeric)\\ndeathdata[['Age Adjusted Death Rate']] = deathdata[['Age Adjusted Death Rate']].apply(pd.to_numeric)\\ndeathdata[['Death Rate']] = deathdata[['Death Rate']].apply(pd.to_numeric)\\ndeathdata.dtypes\\n\\n\\n# In[ ]:\\n\\n\\n# what is the leading cause of death for Asian and Pacific Islanders in 2007?\\n\\n\\n# In[ ]:\\n\\n\\ndeathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='Asian and Pacific Islander')].sort_values('Deaths', ascending=False).head()\\n\\n\\n# In[ ]:\\n\\n\\ndeathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='Black Non-Hispanic')].sort_values('Deaths', ascending=False).head()\\n\\n\\n# In[ ]:\\n\\n\\ndeathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='Hispanic')].sort_values('Deaths', ascending=False).head()\\n\\n\\n# In[ ]:\\n\\n\\ndeathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='Not Stated/Unknown')].sort_values('Deaths', ascending=False).head()\\n\\n\\n# In[ ]:\\n\\n\\ndeathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='Other Race/ Ethnicity')].sort_values('Deaths', ascending=False).head()\\n\\n\\n# In[ ]:\\n\\n\\ndeathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='White Non-Hispanic')].sort_values('Deaths', ascending=False).head()\\n\\n\\n# In[ ]:\\n\\n\\n# Let's examine if and how these numbers changed by 2014:\\n\\n\\n# In[ ]:\\n\\n\\n# what is the leading cause of death for Asian and Pacific Islanders in 2014?\\n\\n\\n# In[ ]:\\n\\n\\ndeathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='Asian and Pacific Islander')].sort_values('Deaths', ascending=False).head()\\n\\n\\n# In[ ]:\\n\\n\\ndeathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='Black Non-Hispanic')].sort_values('Deaths', ascending=False).head()\\n\\n\\n# In[ ]:\\n\\n\\ndeathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='Hispanic')].sort_values('Deaths', ascending=False).head()\\n\\n\\n# In[ ]:\\n\\n\\ndeathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='Not Stated/Unknown')].sort_values('Deaths', ascending=False).head()\\n\\n\\n# In[ ]:\\n\\n\\ndeathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='Other Race/ Ethnicity')].sort_values('Deaths', ascending=False).head()\\n\\n\\n# In[ ]:\\n\\n\\ndeathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='White Non-Hispanic')].sort_values('Deaths', ascending=False).head()\\n\\n\\n# In[ ]:\\n\\n\\n# # International Student Data\\n\\n\\n# In[ ]:\\n\\n\\nstudentdata = pd.read_csv('student_data.csv', encoding='latin-1')\\nstudentdata\\n\\n\\n# In[ ]:\\n\\n\\n# Schema of Dataframes:\\n# Columns in studentdata with example values:\\n# Country (Australia), Region (New South Wales), Gender (Female), Age-years (10), Handed (Right-handed), Height_cm (0.0), Footlength_cm (0.0), Armspan_cm (0.0), Languages_spoken (2.0), Travel_to_School (Car), Travel_time_to_School (10.0), Reaction_time (0.66), Score_in_memory_game (56.0), Favourite_physical_activity (Netball), Importance_reducing_pollution (1000.0), Importance_recycling_rubbish (947.0), Importance_conserving_water (1000.0), Importance_saving_enery (214.0), Importance_owning_computer (1000.0), Importance_Internet_access (947.0)\\n\\n\\n# In[ ]:\\n\\n\\n# what is the number of right or left handed students for each country?\\n\\n\\n# In[ ]:\\n\\n\\nstudentdata.groupby('Country')['Handed'].value_counts()\\n\\n\\n# In[ ]:\\n\\n\\n# what is the total number of right or left handed students?\\n\\n\\n# In[ ]:\\n\\n\\nstudentdata['Handed'].value_counts()\\n\\n\\n# In[ ]:\\n\\n\\n# Problem: which is the most favorite sports for each country?\\n\\n\\n# In[ ]:\\n\\n\\n# Solution:\\n\",\n",
    "            \"turn\": {\n",
    "              \"intent\": {\n",
    "                \"value\": \"# Problem: which is the most favorite sports for each country?\",\n",
    "                \"is_cell_intent\": True,\n",
    "                \"cell_idx\": 46,\n",
    "                \"line_span\": [\n",
    "                  0,\n",
    "                  1\n",
    "                ],\n",
    "                \"not_sure\": None,\n",
    "                \"output_variables\": []\n",
    "              },\n",
    "              \"code\": {\n",
    "                \"value\": \"studentdata.groupby('Country')['Favourite_physical_activity'].value_counts()\",\n",
    "                \"cell_idx\": 47,\n",
    "                \"num_lines\": 1,\n",
    "                \"line_span\": [\n",
    "                  0,\n",
    "                  1\n",
    "                ]\n",
    "              },\n",
    "              \"code_context\": \"# # IS362 - Project Two\\n\\nimport pandas as pd\\nimport numpy as np\\n\\n# # California Cities Data Analysis\\n\\ncalidata = pd.read_csv('california_cities_data.csv')\\ncalidata\\n\\n# rename the first column to city_id\\n\\ncalidata.rename(columns={calidata.columns.values[0]: 'city_id'}, inplace=True)\\ncalidata.head()\\n\\n# What are the top-5 cities with highest elevation?\\n\\ncalidata[[\\\"city\\\", \\\"elevation_ft\\\"]].sort_values(by=[\\\"elevation_ft\\\"], ascending=False).head()\\n\\n# What are the top-5 cities with lowest elevation?\\n\\ncalidata[[\\\"city\\\", \\\"elevation_ft\\\"]].sort_values(by=[\\\"elevation_ft\\\"], ascending=True).head()\\n\\n# What are the top-5 cities with highest population?\\n\\ncalidata[[\\\"city\\\", \\\"population_total\\\"]].sort_values(by=[\\\"population_total\\\"], ascending=False).head()\\n\\n# What are the top-5 cities with lowest population?\\n\\ncalidata[[\\\"city\\\", \\\"population_total\\\"]].sort_values(by=[\\\"population_total\\\"], ascending=True).head()\\n\\ncalidata.loc[(calidata.city== \\\"Pomona\\\"), \\\"population_total\\\"] = 151348\\ncalidata[[\\\"city\\\", \\\"population_total\\\"]].sort_values(by=[\\\"population_total\\\"], ascending=True).head()\\n\\n# What are the top-5 cities with largest area?\\n\\ncalidata[[\\\"city\\\", \\\"area_total_sq_mi\\\"]].sort_values(by=[\\\"area_total_sq_mi\\\"], ascending=False).head()\\n\\n# A quick google search confirms that this is correct.\\n\\n# # NYC Cause of Death Data Set\\n\\n# Let's move on to the Academic Journal data set. This data set contains information about the leading causes of death in New York City, by race and gender.\\n\\ndeathdata = pd.read_csv('nyc_death_data.csv')\\ndeathdata\\n\\ndeathdata = deathdata.replace('.', np.nan)\\n\\ndeathdata.dtypes\\n\\n# Converting object data types to numeric\\n\\ndeathdata[['Deaths']] = deathdata[['Deaths']].apply(pd.to_numeric)\\ndeathdata[['Age Adjusted Death Rate']] = deathdata[['Age Adjusted Death Rate']].apply(pd.to_numeric)\\ndeathdata[['Death Rate']] = deathdata[['Death Rate']].apply(pd.to_numeric)\\ndeathdata.dtypes\\n\\n# what is the leading cause of death for Asian and Pacific Islanders in 2007?\\n\\ndeathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='Asian and Pacific Islander')].sort_values('Deaths', ascending=False).head()\\n\\ndeathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='Black Non-Hispanic')].sort_values('Deaths', ascending=False).head()\\n\\ndeathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='Hispanic')].sort_values('Deaths', ascending=False).head()\\n\\ndeathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='Not Stated/Unknown')].sort_values('Deaths', ascending=False).head()\\n\\ndeathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='Other Race/ Ethnicity')].sort_values('Deaths', ascending=False).head()\\n\\ndeathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='White Non-Hispanic')].sort_values('Deaths', ascending=False).head()\\n\\n# Let's examine if and how these numbers changed by 2014:\\n\\n# what is the leading cause of death for Asian and Pacific Islanders in 2014?\\n\\ndeathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='Asian and Pacific Islander')].sort_values('Deaths', ascending=False).head()\\n\\ndeathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='Black Non-Hispanic')].sort_values('Deaths', ascending=False).head()\\n\\ndeathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='Hispanic')].sort_values('Deaths', ascending=False).head()\\n\\ndeathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='Not Stated/Unknown')].sort_values('Deaths', ascending=False).head()\\n\\ndeathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='Other Race/ Ethnicity')].sort_values('Deaths', ascending=False).head()\\n\\ndeathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='White Non-Hispanic')].sort_values('Deaths', ascending=False).head()\\n\\n# # International Student Data\\n\\nstudentdata = pd.read_csv('student_data.csv', encoding='latin-1')\\nstudentdata\\n\\n# what is the number of right or left handed students for each country?\\n\\nstudentdata.groupby('Country')['Handed'].value_counts()\\n\\n# what is the total number of right or left handed students?\\n\\nstudentdata['Handed'].value_counts()\",\n",
    "              \"delta_code_context\": \"\",\n",
    "              \"metadata\": {\n",
    "                \"annotator_id\": \"gopal\",\n",
    "                \"utterance_without_output_spec\": \"which is the most favorite sports for each country?\"\n",
    "              }\n",
    "            },\n",
    "            \"notebook\": None,\n",
    "            \"metadata\": {\n",
    "              \"context_cells\": [\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"# IS362 - Project Two\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"import pandas as pd\\nimport numpy as np\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"# California Cities Data Analysis\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"calidata = pd.read_csv('california_cities_data.csv')\\ncalidata\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"rename the first column to city_id\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"calidata.rename(columns={calidata.columns.values[0]: 'city_id'}, inplace=True)\\ncalidata.head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"What are the top-5 cities with highest elevation?\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"calidata[[\\\"city\\\", \\\"elevation_ft\\\"]].sort_values(by=[\\\"elevation_ft\\\"], ascending=False).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"What are the top-5 cities with lowest elevation?\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"calidata[[\\\"city\\\", \\\"elevation_ft\\\"]].sort_values(by=[\\\"elevation_ft\\\"], ascending=True).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"What are the top-5 cities with highest population?\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"calidata[[\\\"city\\\", \\\"population_total\\\"]].sort_values(by=[\\\"population_total\\\"], ascending=False).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"What are the top-5 cities with lowest population?\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"calidata[[\\\"city\\\", \\\"population_total\\\"]].sort_values(by=[\\\"population_total\\\"], ascending=True).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"calidata.loc[(calidata.city== \\\"Pomona\\\"), \\\"population_total\\\"] = 151348\\ncalidata[[\\\"city\\\", \\\"population_total\\\"]].sort_values(by=[\\\"population_total\\\"], ascending=True).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"What are the top-5 cities with largest area?\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"calidata[[\\\"city\\\", \\\"area_total_sq_mi\\\"]].sort_values(by=[\\\"area_total_sq_mi\\\"], ascending=False).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"A quick google search confirms that this is correct.\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"# NYC Cause of Death Data Set\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"Let's move on to the Academic Journal data set. This data set contains information about the leading causes of death in New York City, by race and gender.\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata = pd.read_csv('nyc_death_data.csv')\\ndeathdata\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata = deathdata.replace('.', np.nan)\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata.dtypes\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"Converting object data types to numeric\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata[['Deaths']] = deathdata[['Deaths']].apply(pd.to_numeric)\\ndeathdata[['Age Adjusted Death Rate']] = deathdata[['Age Adjusted Death Rate']].apply(pd.to_numeric)\\ndeathdata[['Death Rate']] = deathdata[['Death Rate']].apply(pd.to_numeric)\\ndeathdata.dtypes\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"what is the leading cause of death for Asian and Pacific Islanders in 2007?\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='Asian and Pacific Islander')].sort_values('Deaths', ascending=False).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='Black Non-Hispanic')].sort_values('Deaths', ascending=False).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='Hispanic')].sort_values('Deaths', ascending=False).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='Not Stated/Unknown')].sort_values('Deaths', ascending=False).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='Other Race/ Ethnicity')].sort_values('Deaths', ascending=False).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata[(deathdata['Year']==2007) & (deathdata['Race Ethnicity']=='White Non-Hispanic')].sort_values('Deaths', ascending=False).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"Let's examine if and how these numbers changed by 2014:\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"what is the leading cause of death for Asian and Pacific Islanders in 2014?\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='Asian and Pacific Islander')].sort_values('Deaths', ascending=False).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='Black Non-Hispanic')].sort_values('Deaths', ascending=False).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='Hispanic')].sort_values('Deaths', ascending=False).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='Not Stated/Unknown')].sort_values('Deaths', ascending=False).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='Other Race/ Ethnicity')].sort_values('Deaths', ascending=False).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"deathdata[(deathdata['Year']==2014) & (deathdata['Race Ethnicity']=='White Non-Hispanic')].sort_values('Deaths', ascending=False).head()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"# International Student Data\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"studentdata = pd.read_csv('student_data.csv', encoding='latin-1')\\nstudentdata\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"what is the number of right or left handed students for each country?\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"studentdata.groupby('Country')['Handed'].value_counts()\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"markdown\",\n",
    "                  \"source\": \"what is the total number of right or left handed students?\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_type\": \"code\",\n",
    "                  \"source\": \"studentdata['Handed'].value_counts()\"\n",
    "                }\n",
    "              ],\n",
    "              \"delta_cell_num\": 0,\n",
    "              \"context_cell_num\": 26,\n",
    "              \"intent_text\": \"# Problem: which is the most favorite sports for each country?\",\n",
    "              \"first_cell_index\": 21,\n",
    "              \"prompt_length\": 1768,\n",
    "              \"schema_info\": [\n",
    "                {\n",
    "                  \"cell_idx\": 3,\n",
    "                  \"df_variables\": [\n",
    "                    {\n",
    "                      \"name\": \"calidata\",\n",
    "                      \"line_num\": 1,\n",
    "                      \"value\": None,\n",
    "                      \"schema_representation\": \"# Columns in calidata with example values:\\n# Unnamed: 0 (0), city (Adelanto), latd (34.57611111), longd (-117.4327778), elevation_m (875.0), elevation_ft (2871.0), population_total (31765), area_total_sq_mi (56.027), area_land_sq_mi (56.009), area_water_sq_mi (0.018), area_total_km2 (145.107), area_land_km2 (145.062), area_water_km2 (0.046), area_water_percent (0.03)\"\n",
    "                    }\n",
    "                  ],\n",
    "                  \"cell_schema_representation\": \"# Columns in calidata with example values:\\n# Unnamed: 0 (0), city (Adelanto), latd (34.57611111), longd (-117.4327778), elevation_m (875.0), elevation_ft (2871.0), population_total (31765), area_total_sq_mi (56.027), area_land_sq_mi (56.009), area_water_sq_mi (0.018), area_total_km2 (145.107), area_land_km2 (145.062), area_water_km2 (0.046), area_water_percent (0.03)\\n\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_idx\": 20,\n",
    "                  \"df_variables\": [\n",
    "                    {\n",
    "                      \"name\": \"deathdata\",\n",
    "                      \"line_num\": 1,\n",
    "                      \"value\": None,\n",
    "                      \"schema_representation\": \"# Columns in deathdata with example values:\\n# Year (2014), Leading Cause (Diabetes Mellitus (E10-E14)), Sex (F), Race Ethnicity (Other Race/ Ethnicity), Deaths (11), Death Rate (.), Age Adjusted Death Rate (.)\"\n",
    "                    }\n",
    "                  ],\n",
    "                  \"cell_schema_representation\": \"# Columns in deathdata with example values:\\n# Year (2014), Leading Cause (Diabetes Mellitus (E10-E14)), Sex (F), Race Ethnicity (Other Race/ Ethnicity), Deaths (11), Death Rate (.), Age Adjusted Death Rate (.)\\n\"\n",
    "                },\n",
    "                {\n",
    "                  \"cell_idx\": 41,\n",
    "                  \"df_variables\": [\n",
    "                    {\n",
    "                      \"name\": \"studentdata\",\n",
    "                      \"line_num\": 1,\n",
    "                      \"value\": None,\n",
    "                      \"schema_representation\": \"# Columns in studentdata with example values:\\n# Country (Australia), Region (New South Wales), Gender (Female), Age-years (10), Handed (Right-handed), Height_cm (0.0), Footlength_cm (0.0), Armspan_cm (0.0), Languages_spoken (2.0), Travel_to_School (Car), Travel_time_to_School (10.0), Reaction_time (0.66), Score_in_memory_game (56.0), Favourite_physical_activity (Netball), Importance_reducing_pollution (1000.0), Importance_recycling_rubbish (947.0), Importance_conserving_water (1000.0), Importance_saving_enery (214.0), Importance_owning_computer (1000.0), Importance_Internet_access (947.0)\"\n",
    "                    }\n",
    "                  ],\n",
    "                  \"cell_schema_representation\": \"# Columns in studentdata with example values:\\n# Country (Australia), Region (New South Wales), Gender (Female), Age-years (10), Handed (Right-handed), Height_cm (0.0), Footlength_cm (0.0), Armspan_cm (0.0), Languages_spoken (2.0), Travel_to_School (Car), Travel_time_to_School (10.0), Reaction_time (0.66), Score_in_memory_game (56.0), Favourite_physical_activity (Netball), Importance_reducing_pollution (1000.0), Importance_recycling_rubbish (947.0), Importance_conserving_water (1000.0), Importance_saving_enery (214.0), Importance_owning_computer (1000.0), Importance_Internet_access (947.0)\\n\"\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a Python data science assistant working with the user to enable code completion.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.llama3.tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer(\"../models/llama3/tokenizer.model\")\n",
    "\n",
    "def format_prompt_cells(cells):\n",
    "    prompt_cells = []\n",
    "    for cell in cells:\n",
    "        if cell[\"cell_type\"] == \"markdown\":\n",
    "            lines = [s.strip() for s in cell[\"source\"].split(\"#\") if s.strip()]\n",
    "            lines = [f\"# {s.strip()}\" for l in lines for s in l.split(\"\\n\") if s.strip()]\n",
    "            if prompt_cells and prompt_cells[-1][\"cell_type\"] == \"markdown\":\n",
    "                prompt_cells[-1][\"source\"] += \"\\n\" + \"\\n\".join(lines)\n",
    "            else:\n",
    "                prompt_cells.append({\"cell_type\": \"markdown\", \"source\": \"\\n\".join(lines)})\n",
    "        else:\n",
    "            prompt_cells.append(cell)\n",
    "        num_tokens = len(tokenizer.encode(prompt_cells[-1][\"source\"], bos=False, eos=False))\n",
    "        prompt_cells[-1][\"num_tokens\"] = len(tokenizer.encode(cell[\"source\"], bos=False, eos=False))\n",
    "    return prompt_cells\n",
    "\n",
    "cells = task[\"metadata\"][\"context_cells\"]\n",
    "clean_cells = format_prompt_cells(cells)\n",
    "total_tokens = sum(map(lambda c: c[\"num_tokens\"], clean_cells))\n",
    "previous_tokens = len(tokenizer.encode(task[\"turn\"][\"code_context\"], bos=False, eos=False))\n",
    "code_context = \"\\n\".join(list(map(lambda c: c[\"source\"], clean_cells)))\n",
    "print(\"Num. Tokens\", total_tokens)\n",
    "print(\"Saved Tokens\", previous_tokens - total_tokens)\n",
    "print(code_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import inspect\n",
    "import shutil\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "OUTPUT_DELIMITER = \"***SCHEMA EXTRACTION***\"\n",
    "\n",
    "def get_execution_state(MAX_ROWS=5):\n",
    "    np.random.seed(42)\n",
    "    output = {\"schemas\": {}, \"variables\": {}}\n",
    "    for name, val in globals().items():\n",
    "        if not name.startswith(\"__\"):\n",
    "            if isinstance(val, pd.DataFrame) or isinstance(val, pd.Series):\n",
    "                rows = val.sample(n=MAX_ROWS).head(MAX_ROWS).to_json()\n",
    "                output[\"schemas\"][name] = json.loads(rows)\n",
    "            match = re.search(r\"<class '(\\w+)'>\", str(type(val)))\n",
    "            if match:\n",
    "                output[\"variables\"][name] =  match.group(1)\n",
    "    return output\n",
    "\n",
    "SCHEMA_EXTRACTION_CODE = \"\"\"\n",
    "{code_context}\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "{get_execution_state_str}\n",
    "\n",
    "print(\"{OUTPUT_DELIMITER}\")\n",
    "output_str = json.dumps(get_execution_state())\n",
    "print(output_str)\n",
    "\"\"\"\n",
    "\n",
    "def generate_schema_info(work_dir, code_context):\n",
    "    tmp_work_dir = os.path.join('/tmp/', str(uuid.uuid4()))\n",
    "    shutil.copytree(work_dir, tmp_work_dir)\n",
    "    output = {\"variables\": [], \"schemas\": {}, \"execution_error\": None, \"extraction_error\": None}\n",
    "    get_execution_state_str = inspect.getsource(get_execution_state)\n",
    "    extraction_code = SCHEMA_EXTRACTION_CODE.format(code_context=code_context, get_execution_state_str=get_execution_state_str, OUTPUT_DELIMITER=OUTPUT_DELIMITER)\n",
    "    command = ['python', '-c', extraction_code]\n",
    "    try:\n",
    "        result = subprocess.run(command, cwd=tmp_work_dir, capture_output=True, text=True, check=True)\n",
    "        exec_state_str = result.stdout.rsplit(OUTPUT_DELIMITER, 1)[-1].strip()\n",
    "        exec_state = json.loads(exec_state_str)\n",
    "        for name, data in exec_state[\"schemas\"].items():\n",
    "            output[\"schemas\"][name] = pd.DataFrame(data)\n",
    "        output[\"variables\"] = exec_state[\"variables\"]\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        output[\"execution_error\"] = str(e.stderr)\n",
    "    except Exception as e:\n",
    "        output[\"extraction_error\"] = str(e)\n",
    "    return output\n",
    "\n",
    "code_context = task[\"turn\"][\"code_context\"]\n",
    "work_dir = os.path.join(os.getcwd(), \"..\", \"artifacts\", notebook[\"work_dir\"])\n",
    "exec_state = generate_schema_info(work_dir, code_context)\n",
    "print(exec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(exec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import uuid\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from src.analysis import ASTVisitor\n",
    "\n",
    "def generate_schema_and_execution_info(cells, work_dir):\n",
    "    work_dir = os.path.join(os.getcwd(), \"..\", \"artifacts\", work_dir)\n",
    "    current_dir = os.getcwd()\n",
    "    tmp_work_dir = os.path.join('/tmp/', str(uuid.uuid4()))\n",
    "    shutil.copytree(work_dir, tmp_work_dir)\n",
    "    os.chdir(tmp_work_dir)\n",
    "    shell = InteractiveShell.instance(user_ns={})\n",
    "    max_rows = 20\n",
    "    np.random.seed(42)\n",
    "    visitor = ASTVisitor()\n",
    "    previous_dataframes = set()\n",
    "    execution_outputs = []\n",
    "    for cell in cells:\n",
    "        cell_state = {}\n",
    "        if cell[\"cell_type\"] == \"code\":\n",
    "            code = cell[\"source\"]\n",
    "            code_tree = ast.parse(code)\n",
    "            visitor.visit(code_tree)\n",
    "            result = shell.run_cell(code)\n",
    "            output = result.result if result.success else result.error_before_exec\n",
    "            cell_state[\"output\"] = output\n",
    "            if isinstance(output, pd.DataFrame) or isinstance(output, pd.Series):\n",
    "                num_rows = min(len(output), max_rows)\n",
    "                cell_state[\"output\"] = output.sample(n=num_rows).head(num_rows)\n",
    "            cell_state[\"new_dataframes\"] = set()\n",
    "            cell_state[\"dataframes\"] = {}\n",
    "            cell_state[\"dataframe_sizes\"] = {}\n",
    "            for k, v in shell.user_ns.items():\n",
    "                if k in visitor.dataframes and isinstance(v, pd.DataFrame) or isinstance(v, pd.Series):\n",
    "                    num_rows = min(len(v), max_rows)\n",
    "                    cell_state[\"dataframes\"][k] = v.sample(n=num_rows).head(num_rows)\n",
    "                    cell_state[\"dataframe_sizes\"] = len(v)\n",
    "                    if k in (visitor.dataframes - previous_dataframes):\n",
    "                        cell_state[\"new_dataframes\"].add(k)\n",
    "            previous_dataframes |= visitor.dataframes\n",
    "        execution_outputs.append(cell_state | cell)\n",
    "    os.chdir(current_dir)\n",
    "    return execution_outputs\n",
    "\n",
    "\n",
    "cells = task[\"metadata\"][\"context_cells\"]\n",
    "cells_cleaned = format_prompt_cells(cells)\n",
    "work_dir = os.path.join(os.getcwd(), \"..\", \"artifacts\", notebook[\"work_dir\"])\n",
    "cells_execution_state = generate_schema_and_execution_info(cells_cleaned, work_dir)\n",
    "\n",
    "pprint(cells_execution_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "from models.llama3.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\"../models/llama3/tokenizer.model\")\n",
    "\n",
    "def get_num_tokens(s):\n",
    "    return len(tokenizer.encode(s, bos=False, eos=False))\n",
    "\n",
    "def format_dataframe(df, format_type, max_rows=5):\n",
    "    num_rows = min(max_rows, len(df))\n",
    "    format_options = {\n",
    "        \"default\": lambda d: d.head(num_rows).to_string(index=False),\n",
    "        \"csv\": lambda d: d.head(num_rows).to_csv(index=False),\n",
    "        \"json\": lambda d: d.head(num_rows).to_json(orient='records', lines=True)\n",
    "    }\n",
    "    return format_options[format_type](df)\n",
    "\n",
    "def build_cell_prompts(cells, add_markdown=True, split_into_cells=False, df_format=\"default\", add_outputs=False, add_new_dataframes=False):\n",
    "    cell_prompts = []\n",
    "    for cell in cells:\n",
    "        prompt = []\n",
    "        if cell[\"cell_type\"] == \"markdown\" and add_markdown:\n",
    "            if split_into_cells:\n",
    "                prompt.append(f\"\\n\\nCELL ({cell['cell_type'].upper()}):\")\n",
    "            prompt.append(cell[\"source\"])\n",
    "        elif cell[\"cell_type\"] == \"code\":\n",
    "            if split_into_cells:\n",
    "                prompt.append(f\"\\n\\nCELL ({cell['cell_type'].upper()}):\")\n",
    "            prompt.append(cell[\"source\"])\n",
    "            output_prompt = []\n",
    "            if add_outputs and not isinstance(cell[\"output\"], types.NoneType):\n",
    "                if isinstance(cell[\"output\"], pd.DataFrame) or isinstance(cell[\"output\"], pd.Series):\n",
    "                    output_prompt.append(f\"EXECUTION OUTPUT:\")\n",
    "                    output_prompt.append(format_dataframe(cell[\"output\"], df_format))\n",
    "                else:\n",
    "                    output_prompt.append(\"EXECUTION OUTPUT:\")\n",
    "                    output_prompt.append(str(cell[\"output\"]))\n",
    "            if not split_into_cells:\n",
    "                output_prompt = [\"\\n\\n\"] + [f\"# {l}\" for p in output_prompt for l in p.split(\"\\n\") if l.strip()]\n",
    "                output_prompt.append(\"\\n\")\n",
    "            prompt.append(\"\\n\".join(output_prompt))\n",
    "            new_dataframes_prompt = []\n",
    "            if add_new_dataframes and cell[\"new_dataframes\"]:\n",
    "                new_dataframes_prompt.append(\"NEW DATAFRAMES:\")\n",
    "                for k in cell[\"new_dataframes\"]:\n",
    "                    df = cell[\"dataframes\"][k]\n",
    "                    new_dataframes_prompt.append(f\"NAME: {k}\")\n",
    "                    new_dataframes_prompt.append(f\"HEAD ({len(df)} ROWS):\\n{format_dataframe(df, df_format)}\\n\\n\")\n",
    "            if not split_into_cells:\n",
    "                new_dataframes_prompt = [\"\\n\\n\"] + [f\"# {l}\" for p in new_dataframes_prompt for l in p.split(\"\\n\") if l.strip()]\n",
    "                new_dataframes_prompt.append(\"\\n\")\n",
    "            prompt.append(\"\\n\".join(new_dataframes_prompt))\n",
    "        cell_prompts.append(\"\\n\".join(prompt))\n",
    "    return cell_prompts\n",
    "\n",
    "def get_notebook_context_explanation(split_into_cells, add_outputs, add_new_dataframes):\n",
    "    notebook_base_prompt = [\"This is the user's code and markdown comments of the current working notebook.\"]\n",
    "    if split_into_cells:\n",
    "        notebook_base_prompt.append(\"Each cell is delineated by the string CELL.\")\n",
    "        if add_outputs:\n",
    "             notebook_base_prompt.append(\"The code cells have been executed and added below the cell with the EXECUTION OUTPUT heading.\")\n",
    "        if add_new_dataframes:\n",
    "            notebook_base_prompt.append(\"New dataframes created in a cell have been added with the heading NEW DATAFRAMES. Dataframes may have more rows than displayed.\")\n",
    "    else:\n",
    "        if add_outputs:\n",
    "            notebook_base_prompt.append(\"The code cells have been executed and added below the cell as comments.\")\n",
    "        if add_new_dataframes:\n",
    "            notebook_base_prompt.append(\"New dataframes created in a cell have been added as comments. \\nDataframes may have more rows than displayed.\")\n",
    "    return \"\\n\".join(notebook_base_prompt)\n",
    "\n",
    "\n",
    "def build_notebook_prompt(\n",
    "    cells_execution_state, \n",
    "    explanation=False,\n",
    "    add_markdown=True,\n",
    "    split_into_cells=True, \n",
    "    df_format=\"default\",\n",
    "    add_outputs=True, \n",
    "    add_new_dataframes=False, \n",
    "    append_dataframes=False,\n",
    "    max_prompt_size=7000,\n",
    "    max_context_cells=None\n",
    "):\n",
    "    cell_prompts = build_cell_prompts(\n",
    "        cells_execution_state,\n",
    "        add_markdown,\n",
    "        split_into_cells, \n",
    "        df_format,\n",
    "        add_outputs, \n",
    "        add_new_dataframes\n",
    "    )\n",
    "\n",
    "    current_dataframes = None\n",
    "    for cell in cells_execution_state[::-1]:\n",
    "        if cell[\"cell_type\"] == \"code\":\n",
    "            current_dataframes = cell[\"dataframes\"]\n",
    "            break\n",
    "    \n",
    "    dataframe_prompts = []\n",
    "    if append_dataframes and current_dataframes:\n",
    "        for k, df in current_dataframes.items():\n",
    "            if not k.startswith(\"_\"):\n",
    "                dataframe_prompts.append(f\"\\nNAME: {k}\")\n",
    "                dataframe_prompts.append(f\"HEAD (MAX. {len(df)} ROWS):\\n{format_dataframe(df, df_format)}\")\n",
    "    dfs_prompt = \"\\n\".join([\"\\n\"] + dataframe_prompts)\n",
    "\n",
    "    tokens_left = max_prompt_size\n",
    "    dataframes_prompt_size = get_num_tokens(dfs_prompt)\n",
    "    tokens_left -= dataframes_prompt_size\n",
    "\n",
    "    prompt = []\n",
    "    for i, cell_prompt in enumerate(cell_prompts[::-1]):\n",
    "        if max_context_cells and i >= max_context_cells:\n",
    "            break\n",
    "        cell_prompt_size = get_num_tokens(cell_prompt)\n",
    "        if cell_prompt_size > tokens_left:\n",
    "            break\n",
    "        tokens_left -= cell_prompt_size\n",
    "        prompt.append(cell_prompt)\n",
    "\n",
    "    context_explanation = get_notebook_context_explanation(split_into_cells, add_outputs, add_new_dataframes) \n",
    "    \n",
    "    total_context_cells = len(cell_prompts)\n",
    "    num_context_cells = len(prompt)\n",
    "    context_prompt = '\\n'.join(prompt[::-1])\n",
    "    notebook_context_prompt = [\"###NOTEBOOK CONTEXT###\", context_prompt.strip()]\n",
    "\n",
    "    if explanation:\n",
    "        notebook_context_prompt = [\"###NOTEBOOK CONTEXT###\", context_explanation, \"\\n\", context_prompt.strip()]\n",
    "    if dataframe_prompts:\n",
    "        notebook_context_prompt.append(\"\\n###AVAILABLE DATAFRAMES###\")\n",
    "        if explanation:\n",
    "            notebook_context_prompt.append(\"These are the available dataframes that have been created.\")\n",
    "        notebook_context_prompt.append(dfs_prompt)\n",
    "\n",
    "    prompt = \"\\n\".join(notebook_context_prompt)\n",
    "    prompt = re.sub(r'\\n{3,}', '\\n\\n', prompt)\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"num_context_cells\": num_context_cells,\n",
    "        \"total_context_cells\": total_context_cells,\n",
    "        \"prompt_size\": get_num_tokens(prompt)\n",
    "    }\n",
    "\n",
    "\n",
    "results = build_notebook_prompt(\n",
    "    cells_execution_state,\n",
    "    explanation=False,\n",
    "    add_markdown=False,\n",
    "    split_into_cells=False,\n",
    "    df_format=\"default\", \n",
    "    add_outputs=False, \n",
    "    add_new_dataframes=False, \n",
    "    append_dataframes=False,\n",
    "    max_prompt_size=7000,\n",
    "    max_context_cells=None\n",
    ") \n",
    "\n",
    "notebook_prompt = results[\"prompt\"]\n",
    "print(results[\"prompt\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent + Task Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_prompt = f\"\"\"\n",
    "###USER QUERY###\n",
    "{task[\"turn\"][\"intent\"][\"value\"]}\n",
    "\"\"\"\n",
    "\n",
    "print(intent_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_prompt = \"\"\"\n",
    "###OBJECTIVE###\n",
    "Your task is to answer the USER QUERY by with the provided NOTEBOOK CONTEXT.\n",
    "You MUST output Pandas Python code that will be parsed and executed in a stateful Jupyter notebook environment.\n",
    "I will tip $10,000,000 if your code is clean and correct.\n",
    "\"\"\"\n",
    "\n",
    "# step_by_step=False\n",
    "# tip=False\n",
    "\n",
    "\"\"\"\n",
    "You MUST output Pandas Python code that will be parsed and executed in a stateful Jupyter notebook environment.\n",
    "You MUST output all text that is not code as comments. Use Pandas methods where applicable.\n",
    "Take a deep breath and think step by step. Ensure your code is logical and accurate to answer the user query.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "{objective_prompt.strip()}\n",
    "\n",
    "{notebook_prompt.strip()}\n",
    "\n",
    "{intent_prompt.strip()}\n",
    "\"\"\"\n",
    "print(prompt.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm import generate_response\n",
    "\n",
    "print(get_num_tokens(prompt))\n",
    "\n",
    "def extract_first_text_between_backticks(text):\n",
    "    pattern = r'```(?:python\\s+)?(.*?)(?:```|$)'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else text\n",
    "\n",
    "response = generate_response(\"LLAMA3_INSTRUCT_8B\", prompt, 0.6)\n",
    "\n",
    "print(extract_first_text_between_backticks(response))\n",
    "print(get_num_tokens(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def generate_execution_metadata():\n",
    "    dataset_path = os.path.join(\"..\", \"datasets\", \"dataset.json\")\n",
    "    output_path = os.path.join(\"..\", \"datasets\", \"dataset.execution_info.pkl\")\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        dataset = json.load(f)\n",
    "    for notebook in dataset:\n",
    "        work_dir = notebook[\"work_dir\"]\n",
    "        for task in notebook[\"turns\"]:\n",
    "            cells = task[\"metadata\"][\"context_cells\"]\n",
    "            formatted_cells = format_prompt_cells(cells)\n",
    "            task[\"metadata\"][\"context_cells_formatted\"] = formatted_cells\n",
    "            task[\"metadata\"][\"execution_state\"] = generate_schema_and_execution_info(formatted_cells, work_dir)\n",
    "            break\n",
    "        break\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "generate_execution_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_execution_metadata():\n",
    "    output_path = os.path.join(\"..\", \"datasets\", \"dataset.execution_info.pkl\")\n",
    "    with open(output_path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return dataset\n",
    "\n",
    "OBJECTIVE_PROMPT = \"\"\"\n",
    "###OBJECTIVE###\n",
    "Your task is to answer the USER QUERY by with the provided NOTEBOOK CONTEXT.\n",
    "You MUST output Pandas Python code that will be parsed and executed in a stateful Jupyter notebook environment.\n",
    "I will tip $10,000,000 if your code is clean and correct.\n",
    "\"\"\"\n",
    "\n",
    "INTENT_PROMPT = \"\"\"\n",
    "###USER QUERY###\n",
    "{user_intent}\n",
    "\"\"\"\n",
    "\n",
    "notebook_prompt_config = dict(\n",
    "    explanation=False,\n",
    "    add_markdown=True,\n",
    "    split_into_cells=True,\n",
    "    df_format=\"default\", \n",
    "    add_outputs=False, \n",
    "    add_new_dataframes=False, \n",
    "    append_dataframes=False,\n",
    "    max_prompt_size=7000,\n",
    "    max_context_cells=None\n",
    ")\n",
    "\n",
    "def build_prompts(notebook_prompt_config):\n",
    "    dataset = read_execution_metadata()\n",
    "    for notebook in dataset:\n",
    "        for task in notebook[\"turns\"]:\n",
    "            cell_execution_info = task[\"metadata\"][\"execution_state\"]\n",
    "            user_intent = task[\"turn\"][\"intent\"][\"value\"]\n",
    "            notebook_prompt = build_notebook_prompt(cell_execution_info, **notebook_prompt_config)\n",
    "            prompt = [\n",
    "                objective_prompt,\n",
    "                notebook_prompt[\"prompt\"],\n",
    "                intent_prompt.format(user_intent=user_intent)\n",
    "            ]\n",
    "            print(\"\\n\".join(prompt))\n",
    "            break\n",
    "        break\n",
    "\n",
    "build_prompts(notebook_prompt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.dataset import generate_execution_metadata, generate_execution_metadata_main\n",
    "\n",
    "# generate_execution_metadata()\n",
    "\n",
    "# # generate_execution_metadata_main()\n",
    "\n",
    "import subprocess\n",
    "\n",
    "\n",
    "python_path = \"/home/sanjeet/FYP\"\n",
    "os.environ[\"PYTHONPATH\"] = python_path\n",
    "\n",
    "subprocess.run([\"python\", \"-m\", \"src.dataset.generate_execution_metadata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataframe with more rows\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Eva', 'Frank', 'Grace', 'Hannah', 'Ian', 'Jack'],\n",
    "    'Age': [24, 30, 22, 35, 28, 29, 33, 31],\n",
    "    'City': ['New York', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataframe(df, max_rows=3):\n",
    "    if len(df) > max_rows:\n",
    "        df_str = df.sample(n=max_rows, random_state=123).head(max_rows).sort_index()\n",
    "        df_str = df_str.to_string() + f\"\\n[showing {max_rows} out of {len(df)} sample rows]\\n\"\n",
    "    else:\n",
    "        df_str = df.to_string()\n",
    "    return df_str\n",
    "    \n",
    "\n",
    "# Set display options\n",
    "# pd.set_option('display.max_rows', 5)  # display only 5 rows\n",
    "# pd.set_option('display.max_columns', 3)  # display only 3 columns\n",
    "\n",
    "# # Get the DataFrame as a string\n",
    "# df_string = df.to_string(max_rows=pd.get_option('display.max_rows'),\n",
    "#                          max_cols=pd.get_option('display.max_columns'))\n",
    "# print(df_string)\n",
    "print(format_dataframe(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > max_rows:\n",
    "    df.sample(max_rows).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import generate_schema_and_execution_info, format_prompt_cells\n",
    "import os\n",
    "import json\n",
    "\n",
    "dataset_path = os.path.join(\"..\", \"datasets\", \"dataset.json\")\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "for notebook in dataset:\n",
    "    work_dir = notebook[\"work_dir\"]\n",
    "    for task in notebook[\"turns\"]:\n",
    "        cells = task[\"metadata\"][\"context_cells\"]\n",
    "        formatted_cells = format_prompt_cells(cells)\n",
    "        task[\"metadata\"][\"context_cells_formatted\"] = formatted_cells\n",
    "        task[\"metadata\"][\"execution_state\"] = generate_schema_and_execution_info(formatted_cells, work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.llama3.tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer(\"models/llama3/tokenizer.model\")\n",
    "\n",
    "\n",
    "def get_num_tokens(s):\n",
    "    return len(tokenizer.encode(s, bos=False, eos=False))\n",
    "\n",
    "def format_dataframe_str(df, original_size=None, max_rows=None, max_tokens=None):\n",
    "    if max_rows and len(df) > max_rows:\n",
    "        df_str = df.sample(max_rows, random_state=123).sort_index().to_string()\n",
    "    else:\n",
    "        df_str = df.to_string()\n",
    "    num_tokens = get_num_tokens(df_str) \n",
    "    if max_tokens and num_tokens > max_tokens:\n",
    "        df_row_strs = df_str.split(\"\\n\")\n",
    "        df_row_tokens = list(map(get_num_tokens, df_row_strs))\n",
    "        df_strs = [df_row_strs[0]]\n",
    "        tot_tokens = df_row_tokens[0]\n",
    "        for row, num_tokens in zip(df_row_strs[1:], df_row_tokens[1:]):\n",
    "            if tot_tokens > max_tokens:\n",
    "                break\n",
    "            tot_tokens += num_tokens\n",
    "            df_strs.append(row)\n",
    "        df_str = \"\\n\".join(df_strs)\n",
    "    original_size = original_size if original_size else len(df)\n",
    "    num_rows = len(df_str.split(\"\\n\")) - 1\n",
    "    if num_rows < original_size:\n",
    "        df_str += f\"\\n[showing {num_rows} sample rows out of {original_size} rows]\"\n",
    "    return df_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"\"\"\n",
    "index,Classification,Year,Aggregate Level,Is Leaf Code,Reporter Code,Reporter,Reporter ISO,Partner Code,Partner,Partner ISO,Commodity Code,Commodity,Qty Unit Code,Qty Unit,Qty,Netweight (kg),Trade Value (US$)\n",
    "0,S4,2011,1,0,643,Russian Federation,RUS,136,Cayman Isds,CYM,6,Manufactured goods classified chiefly by material,1,No Quantity,,,75591\n",
    "1,S4,2012,1,0,643,Russian Federation,RUS,136,Cayman Isds,CYM,6,Manufactured goods classified chiefly by material,1,No Quantity,,,27778\n",
    "2,S4,2015,1,0,643,Russian Federation,RUS,136,Cayman Isds,CYM,6,Manufactured goods classified chiefly by material,1,No Quantity,,,2131579\n",
    "3,S4,2016,1,0,643,Russian Federation,RUS,136,Cayman Isds,CYM,6,Manufactured goods classified chiefly by material,1,No Quantity,0,0,9055543\n",
    "4,S4,2017,1,0,643,Russian Federation,RUS,136,Cayman Isds,CYM,6,Manufactured goods classified chiefly by material,1,No Quantity,0,0,19952598\n",
    "5,S4,2018,1,0,643,Russian Federation,RUS,136,Cayman Isds,CYM,6,Manufactured goods classified chiefly by material,1,No Quantity,0,0,14199947\n",
    "6,S4,2019,1,0,643,Russian Federation,RUS,136,Cayman Isds,CYM,6,Manufactured goods classified chiefly by material,1,No Quantity,0,0,13773062\n",
    "7,S4,2020,1,0,643,Russian Federation,RUS,136,Cayman Isds,CYM,6,Manufactured goods classified chiefly by material,1,No Quantity,0,0,21018466\n",
    "8,S4,2014,1,0,643,Russian Federation,RUS,136,Cayman Isds,CYM,8,Miscellaneous manufactured articles,1,No Quantity,,,16098\n",
    "9,S4,2017,1,0,643,Russian Federation,RUS,136,Cayman Isds,CYM,8,Miscellaneous manufactured articles,1,No Quantity,0,0,5398\n",
    "10,S4,2018,1,0,643,Russian Fxederation,RUS,136,Cayman Isds,CYM,8,Miscellaneous manufactured articles,1,No Quantity,0,0,1279\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "df = pd.read_csv(StringIO(abc))\n",
    "\n",
    "# print(format_dataframe_str(df, max_tokens=200))\n",
    "\n",
    "\n",
    "\n",
    "get_num_tokens(\"\"\"# country                          object\n",
    "# beer_servings                     int64\n",
    "# spirit_servings                   int64\n",
    "# wine_servings                     int64\n",
    "# total_litres_of_pure_alcohol    float64\n",
    "# continent                        object\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/sanjeet/FYP\")\n",
    "\n",
    "from src.prompts import build_prompts_dataset\n",
    "\n",
    "notebook_prompt_config = dict(\n",
    "    add_markdown=True,\n",
    "    split_into_cells=False, \n",
    "    add_outputs=False, \n",
    "    add_new_dataframes=False, \n",
    "    append_dataframes=False,\n",
    "    max_dataframes = 15,\n",
    "    prioritize_dataframes = True,\n",
    "    max_prompt_size=7000,\n",
    "    max_context_cells=None\n",
    ")\n",
    "\n",
    "prompts = build_prompts_dataset(notebook_prompt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_task = prompts[0][\"turns\"][0]\n",
    "max_notebook = None\n",
    "ti = None\n",
    "\n",
    "\n",
    "for notebook in prompts:\n",
    "    for i, task in enumerate(notebook[\"turns\"]):\n",
    "        if max_task[\"prompt_size\"] < task[\"prompt_size\"]:\n",
    "            max_task = task\n",
    "            max_notebook = notebook\n",
    "            ti = i\n",
    "\n",
    "print(max_task[\"prompt_size\"], ti, max_notebook[\"notebook_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(notebook[\"notebook_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = random.randint(0, len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "os.chdir(\"/root/FYP\")\n",
    "\n",
    "from src.execution import get_execution_state\n",
    "from src.prompts import build_notebook_prompt, build_task_prompt\n",
    "\n",
    "with open(os.path.join(\"datasets\", \"dataset.formatted.json\")) as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "notebook_name = dataset[ni][\"notebook_name\"]\n",
    "print(notebook_name)\n",
    "# notebook_name = \"dataset_tmdb-movies/notebook_0/annotated.ipynb\"\n",
    "task_index = -1\n",
    "task = dataset[ni][\"turns\"][task_index]\n",
    "\n",
    "# for n in dataset:\n",
    "#     if n[\"notebook_name\"] == notebook_name:\n",
    "#         task = n[\"turns\"][task_index]\n",
    "\n",
    "\n",
    "exec_info = get_execution_state(notebook_name)\n",
    "cell_info = exec_info[len(exec_info)-1]\n",
    "\n",
    "notebook_prompt_config =  dict(\n",
    "    add_markdown=True,\n",
    "    split_into_cells=False, \n",
    "    add_outputs=False, \n",
    "    add_new_dataframes=True, \n",
    "    append_dataframes=False,\n",
    "    max_dataframes = 15,\n",
    "    prioritize_dataframes = True,\n",
    "    max_prompt_size=7000,\n",
    "    max_context_cells=None\n",
    ")\n",
    "\n",
    "# out = build_notebook_prompt(cell_info, **notebook_prompt_config)\n",
    "\n",
    "prompt, out = build_task_prompt(task, cell_info, notebook_prompt_config)\n",
    "\n",
    "print(prompt[-1][\"content\"])\n",
    "\n",
    "print(\"\\n###Answer###\\n\", task[\"turn\"][\"code\"][\"value\"])\n",
    "# print(out[\"notebook_prompt_size\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"LLAMA3_INSTRUCT_8B\", \"LLAMA3_INSTRUCT_70B\"\n",
    "\n",
    "from src.llm import generate_response\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"------------ANSWER-------------\")\n",
    "    print(generate_response(\"LLAMA3_INSTRUCT_70B\", messages, 0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/root/FYP\")\n",
    "\n",
    "from src.experiments import generate_dataset\n",
    "\n",
    "generate_dataset(\"fyp.with_created_dataframes_new_prompt\", notebook_prompt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cell_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OBJECTIVE_PROMPT = \"\"\"\n",
    "###OBJECTIVE###\n",
    "Your task is to answer the USER QUERY by with the provided NOTEBOOK CONTEXT.\n",
    "You MUST output Pandas Python code that will be parsed and executed in a stateful Jupyter notebook environment.\n",
    "I will tip $10,000,000 if your code is clean and correct.\n",
    "\"\"\"\n",
    "\n",
    "INTENT_PROMPT = \"\"\"\n",
    "###USER QUERY###\n",
    "{user_intent}\n",
    "\"\"\"\n",
    "\n",
    "user_intent = max_task[\"turn\"][\"intent\"][\"value\"]\n",
    "\n",
    "prompt = [\n",
    "    OBJECTIVE_PROMPT,\n",
    "    out[\"notebook_prompt\"],\n",
    "    INTENT_PROMPT.format(user_intent=user_intent)\n",
    "]\n",
    "\n",
    "print(\"\\n\".join(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cell_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join([l for l in max_task[\"input\"].split(\"\\n\") if (not l.startswith(\"#\") and l.strip())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis import ASTVisitor, DATAFRAME_RETURN_FUNCTIONS, DATFRAME_DERIVE_FUNCTIONS\n",
    "import ast\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "code = \"\"\"\n",
    "import pandas as pd\n",
    "import pandas\n",
    "\n",
    "# Step 1. Import the necessary libraries\n",
    "import pandas as pd\n",
    "# Step 2. Import the dataset from drinks.csv\n",
    "# Step 3. Assign it to a variable called drinks.\n",
    "drinks = pd.read_csv('drinks.csv')\n",
    "# Step 4. Which continent drinks more beer on average?\n",
    "drinks.groupby('continent')['beer_servings'].mean()\n",
    "# The continent which drinks more beers is Europe (EU).\n",
    "drinks.groupby('continent')['wine_servings'].describe()\n",
    "# Again, the continent where people drink more wine is Europe.\n",
    "# Step 6. Print the mean alcohol consumption per continent for every column\n",
    "drinks.groupby('continent').mean()\n",
    "# Overall, Europe is still being the continent which drinks more alcohol, probably because of Ireland (kidding!).\n",
    "# Step 7. Print the median alcohol consumption per continent for every column\n",
    "drinks.groupby('continent').median()\n",
    "\"\"\"\n",
    "\n",
    "from src.prompts import select_dataframes_by_priority\n",
    "\n",
    "code = \"\"\"\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "df = pd.read_csv('NYC_Restaurants.csv', dtype=str)\n",
    "df['RESTAURANT'] = df['DBA'] + ' '+ df['BUILDING'] + ' '+ df['STREET'] + ' '+ df['ZIPCODE']\n",
    "df_noduplicates = df.drop_duplicates(subset='RESTAURANT')\n",
    "print(len(df_noduplicates))\n",
    "vc = (df_noduplicates['DBA'].value_counts())\n",
    "df1 = vc.to_frame(name='valuecounts')\n",
    "chains  = df1[df1.valuecounts > 1]\n",
    "num_chains = len(chains)\n",
    "top_20_chains = chains[:20]\n",
    "top_20_chains.plot(kind = 'bar')\n",
    "df1\n",
    "notchains  = df1[df1.valuecounts ==1]\n",
    "len(notchains)\n",
    "fr_chns = 1.0-(1.0*len(notchains)/len(df_noduplicates))\n",
    "df_notchains = df_noduplicates.groupby(\"DBA\").filter(lambda x: len(x) == 1)\n",
    "boro_notchain_pivot = pd.pivot_table (df_notchains, index = 'BORO', values = 'RESTAURANT', aggfunc = lambda x: len(x.unique()))\n",
    "boro_notchain_pivot.query('BORO != [\"Missing\"]').plot(kind = \"bar\")\n",
    "boro_restaurant_pivot = pd.pivot_table (df_noduplicates, index = 'BORO', values = 'RESTAURANT', aggfunc = lambda x: len(x.unique()))\n",
    "boro_notchain_pivot = boro_notchain_pivot.query('BORO != [\"Missing\"]')\n",
    "boro_restaurant_pivot = boro_restaurant_pivot.query('BORO != [\"Missing\"]')\n",
    "boro_notchain_pivot['TOTAL RESTAURANTS'] = boro_restaurant_pivot\n",
    "fraction = boro_notchain_pivot.div(boro_notchain_pivot['TOTAL RESTAURANTS'], axis='index')\n",
    "fraction = fraction.drop('TOTAL RESTAURANTS', 1)\n",
    "fraction.columns.values[0] = 'Not Chain Fraction'\n",
    "fraction.plot(kind = \"bar\")\n",
    "cuis = df_noduplicates['CUISINE DESCRIPTION'].value_counts()\n",
    "cuis[:20].plot(kind='bar')\n",
    "mask = (df['VIOLATION CODE']).isnull()\n",
    "no_violations = df[mask]\n",
    "no_violations[['CUISINE DESCRIPTION', 'RESTAURANT']]\n",
    "cuisine_no_violations = no_violations['CUISINE DESCRIPTION'].value_counts()\n",
    "cuisine_no_violations[:20].plot(kind='bar')\n",
    "mask = (df['VIOLATION CODE']).notnull()\n",
    "violations = df[mask]\n",
    "cuisine_violations = violations['CUISINE DESCRIPTION'].value_counts()\n",
    "total_cuisine = pd.concat([cuisine_violations , cuisine_no_violations], axis = 1)\n",
    "total_cuisine.columns = ['VIOLATIONS', 'NO VIOLATIONS']\n",
    "total_cuisine = total_cuisine.fillna(0)\n",
    "total_cuisine['TOTAL INSTANCES'] = total_cuisine['VIOLATIONS'] + total_cuisine['NO VIOLATIONS']\n",
    "new_mask = (total_cuisine['TOTAL INSTANCES'] >= 20)\n",
    "total_cuisine = total_cuisine[new_mask]\n",
    "total_cuisine['PERCENT NO VIOLATIONS'] = total_cuisine['NO VIOLATIONS']*100 / total_cuisine['TOTAL INSTANCES']\n",
    "total_cuisine.sort_values(['PERCENT NO VIOLATIONS'], ascending=False)\n",
    "violations = pd.crosstab(df['BORO'], df['VIOLATION DESCRIPTION']).query('BORO != [\"Missing\"]')\n",
    "vstack = violations.stack()\n",
    "violations2 = vstack.unstack('BORO')\n",
    "mostcommon = DataFrame({'Most Common Complaint':violations2.idxmax(),'Number of Complaints':violations2.max()}) \n",
    "mostcommon\n",
    "vtotals = df['VIOLATION DESCRIPTION'].value_counts()\n",
    "violations2['VIOLATION FREQUENCY'] = vtotals\n",
    "norm = violations2.div(violations2['VIOLATION FREQUENCY'], axis='index')\n",
    "normalized = norm.drop('VIOLATION FREQUENCY', 1)\n",
    "mostcommon2 = DataFrame({'Most Common Complaint':normalized.idxmax(),'Number of Complaints':normalized.max()}) \n",
    "mostcommon2\n",
    "\"\"\"\n",
    "\n",
    "select_dataframes_by_priority(code, max_dataframes=15, display=True)\n",
    "\n",
    "# code = max_task[\"input\"]\n",
    "\n",
    "# visitor = ASTVisitor()\n",
    "# visitor.record_calls = True\n",
    "# visitor.visit(ast.parse(code))\n",
    "# print(visitor.function_calls)\n",
    "# print(visitor.dataframes)\n",
    "# print(json.dumps(visitor.dataframe_graph, indent=2, default=list))\n",
    "# pprint(visitor.dataframe_graph_edges)\n",
    "\n",
    "# import networkx as nx\n",
    "# G_graph = nx.DiGraph()\n",
    "\n",
    "# for node, edges in visitor.dataframe_graph.items():\n",
    "#     for edge in edges:\n",
    "#         G_graph.add_edge(node, edge)\n",
    "        \n",
    "# layout = nx.circular_layout(G_graph)\n",
    "# nodes_no_incoming_edges = [node for node in G_graph.nodes() if len(list(G_graph.predecessors(node))) == 0]\n",
    "# nodes_multiple_incoming_edges = [node for node in G_graph.nodes() if len(list(G_graph.predecessors(node))) > 1]\n",
    "\n",
    "# fig = plt.figure()\n",
    "\n",
    "# nx.draw(G_graph, pos=layout, with_labels=True, font_size=10)\n",
    "# nx.draw_networkx_nodes(G_graph, pos=layout, nodelist=nodes_no_incoming_edges, node_color='red', alpha=0.9)\n",
    "# nx.draw_networkx_nodes(G_graph, pos=layout, nodelist=nodes_multiple_incoming_edges, node_color='green', alpha=0.9)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.use(\"pgf\")\n",
    "# matplotlib.rcParams.update({\n",
    "#     \"pgf.texsystem\": \"pdflatex\",\n",
    "#     'font.family': 'serif',\n",
    "#     'text.usetex': True,\n",
    "#     'pgf.rcfonts': False,\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/root/FYP\")\n",
    "from src.prompts import select_dataframes_by_priority\n",
    "\n",
    "code = \"\"\"\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('NYC_Restaurants.csv', dtype=str)\n",
    "\n",
    "df_noduplicates = df.drop_duplicates(subset='RESTAURANT')\n",
    "df_notchains = df_noduplicates.groupby(\"DBA\").filter(lambda x: len(x) == 1)\n",
    "boro_notchain_pivot = pd.pivot_table(df_notchains, index = 'BORO', values = 'RESTAURANT', aggfunc = lambda x: len(x.unique()))\n",
    "boro_restaurant_pivot = pd.pivot_table(df_noduplicates, index = 'BORO', values = 'RESTAURANT', aggfunc = lambda x: len(x.unique()))\n",
    "boro_notchain_pivot['TOTAL RESTAURANTS'] = boro_restaurant_pivot\n",
    "\n",
    "cuis = df_noduplicates['CUISINE DESCRIPTION'].value_counts()\n",
    "mask = (df['VIOLATION CODE']).isnull()\n",
    "no_violations = df[mask]\n",
    "no_violations[['CUISINE DESCRIPTION', 'RESTAURANT']]\n",
    "\n",
    "cuisine_no_violations = no_violations['CUISINE DESCRIPTION'].value_counts()\n",
    "mask = (df['VIOLATION CODE']).notnull()\n",
    "violations = df[mask]\n",
    "cuisine_violations = violations['CUISINE DESCRIPTION'].value_counts()\n",
    "\n",
    "total_cuisine = pd.concat([cuisine_violations , cuisine_no_violations], axis = 1)\n",
    "violations = pd.crosstab(df['BORO'], df['VIOLATION DESCRIPTION']).query('BORO != [\"Missing\"]')\n",
    "\n",
    "vstack = violations.stack()\n",
    "violations2 = vstack.unstack('BORO')\n",
    "mostcommon = DataFrame({'Most Common Complaint':violations2.idxmax(),'Number of Complaints':violations2.max()}) \n",
    "\"\"\"\n",
    "\n",
    "select_dataframes_by_priority(code, max_dataframes=12, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complex_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLEX_DATAFRAME_METHODS = frozenset({'aggregate', 'crosstab', 'pivot_table', 'cut', 'concat', 'qcut', 'merge', 'join', 'agg', 'melt', 'groupby'})\n",
    "\n",
    "def display_dataframme_graph(code):\n",
    "    visitor = ASTVisitor()\n",
    "    visitor.record_calls = True\n",
    "    visitor.visit(ast.parse(code))\n",
    "    G = nx.DiGraph()\n",
    "    for edge, calls in visitor.dataframe_graph_edges.items():\n",
    "        G.add_edge(*edge, calls=set(calls))\n",
    "    return G, visitor\n",
    "\n",
    "def select_dataframes_by_priority(code, max_dataframes=10):\n",
    "    visitor = ASTVisitor()\n",
    "    visitor.record_calls = True\n",
    "    visitor.visit(ast.parse(code))\n",
    "    print(\"YO\", visitor.dataframe_graph_edges)\n",
    "    G = nx.DiGraph()\n",
    "    for edge, calls in visitor.dataframe_graph_edges.items():\n",
    "        G.add_edge(*edge, calls=set(calls))\n",
    "    base_dfs = set([v for u, v in G.edges() if u == \"[SOURCE]\"])\n",
    "    print(\"BRUH\", base_dfs)\n",
    "    complex_dfs = set()\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if data[\"calls\"].intersection(COMPLEX_DATAFRAME_METHODS):\n",
    "            complex_dfs.add(v)\n",
    "    print(\"BRUH\", complex_dfs)\n",
    "    dfs = base_dfs.union(complex_dfs)\n",
    "    max_remaining_dfs = max_dataframes - len(base_dfs) - len(complex_dfs)\n",
    "    if max_remaining_dfs > 0:\n",
    "        reverse_bfs_order = list(nx.bfs_tree(G, next(iter(base_dfs))).nodes)[::-1]\n",
    "        for df in reverse_bfs_order:\n",
    "            if max_remaining_dfs <= 0:\n",
    "                break\n",
    "            if df not in dfs:\n",
    "                dfs.add(df)\n",
    "                max_remaining_dfs -= 1\n",
    "    all_dataframes = list(dict.fromkeys(visitor.dataframes))\n",
    "    dfs_order_index = {i: df for i, df in enumerate(all_dataframes) if df in dfs}\n",
    "    ordered_dfs = list(dict(sorted(dfs_order_index.items())).values())\n",
    "    if len(dfs) > max_dataframes:\n",
    "        ordered_dfs = ordered_dfs[-max_dataframes:]\n",
    "    return ordered_dfs\n",
    "\n",
    "print(select_dataframes_by_priority(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(DATFRAME_DERIVE_FUNCTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_pandas_dataframe_return_methods())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "class AssignVisitor(ast.NodeVisitor):\n",
    "    def visit_Assign(self, node):\n",
    "        if isinstance(node.targets[0], ast.Name):\n",
    "            lhs = [target.id for target in node.targets]\n",
    "            rhs = node.value\n",
    "        elif isinstance(node.targets[0], ast.Tuple):\n",
    "            lhs = [elt.id for target in node.targets for elt in target.elts]\n",
    "            rhs = [elt for elt in node.value.elts]\n",
    "        print(\"LHS:\", lhs)\n",
    "        print(\"RHS:\", rhs)\n",
    "        print(\"------------------\")\n",
    "\n",
    "\n",
    "code = \"\"\"\n",
    "x = 10\n",
    "y = x + 5\n",
    "z, w = 20, 30\n",
    "\"\"\"\n",
    "tree = ast.parse(code)\n",
    "visitor = AssignVisitor()\n",
    "visitor.visit(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_callable_methods(module):\n",
    "    return [\n",
    "        m for m in dir(module) \n",
    "        if callable(getattr(module, m)) and not m.startswith(\"_\")\n",
    "    ]\n",
    "\n",
    "missed = []\n",
    "\n",
    "def get_pandas_dataframe_return_methods():\n",
    "    RETURN_TYPE_FILTER = frozenset([\"DataFrame\", \"Series\", \"NDFrame\", \"NDFrameT\"])\n",
    "    MISC_ATTRIBUTES = [\"DataFrame\",\"Series\",\"iloc\", \"loc\", \"ix\", \"at\", \"iat\", \"groupby\"]\n",
    "    res = set(MISC_ATTRIBUTES)\n",
    "    for module in [pd, pd.DataFrame, pd.Series]:\n",
    "        methods = get_callable_methods(module)\n",
    "        for m in methods:\n",
    "            c = getattr(module, m)\n",
    "            if hasattr(c, \"__annotations__\"):\n",
    "                if \"return\" in c.__annotations__:\n",
    "                    return_types = c.__annotations__[\"return\"]\n",
    "                    for t in RETURN_TYPE_FILTER:\n",
    "                        if t in str(return_types):\n",
    "                            res.add(m)\n",
    "                            break\n",
    "                else:\n",
    "                    missed.append((m, c))\n",
    "\n",
    "get_pandas_dataframe_return_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis import get_pandas_dataframe_return_methods\n",
    "\n",
    "ms = get_pandas_dataframe_return_methods()\n",
    "\n",
    "base_url = \"https://pandas.pydata.org/docs/reference/api/\"\n",
    "\n",
    "urls = []\n",
    "\n",
    "for module in [pd, pd.DataFrame, pd.Series]:\n",
    "    methods = get_callable_methods(module)\n",
    "    for m in methods:\n",
    "        base_name = module.__name__\n",
    "        if module.__name__ != \"pandas\":\n",
    "            base_name = f\"pandas.{base_name}\"\n",
    "        url = f\"{base_url}{base_name}.{m}.html\"\n",
    "        urls.append(url)\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.execution import generate_execution_metadata\n",
    "\n",
    "generate_execution_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cells = -1\n",
    "\n",
    "for n in dataset:\n",
    "    max_cells = max(max_cells, len(n[\"turns\"][-1][\"metadata\"][\"context_cells_formatted\"]))\n",
    "max_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_context = \"\"\"\n",
    "df = pd.read_csv()\n",
    "\"\"\"\n",
    "\n",
    "code = \"\"\"\n",
    "len(df[~df.AtLeast2Issues & df.Treatment]) / df.Treatment.sum()\n",
    "\"\"\"\n",
    "\n",
    "from src.analysis import analyse_code\n",
    "import ast\n",
    "\n",
    "res = analyse_code(code_context, code)\n",
    "\n",
    "print(res[\"used_dataframes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "os.chdir(\"/root/FYP\")\n",
    "\n",
    "from src.execution import get_execution_state\n",
    "from src.prompts import add_new_dataframe_cells\n",
    "\n",
    "with open(os.path.join(\"datasets\", \"dataset.formatted.json\")) as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "ni = 123\n",
    "notebook_name = dataset[ni][\"notebook_name\"]\n",
    "task_index = -1\n",
    "task = dataset[ni][\"turns\"][task_index]\n",
    "exec_info = get_execution_state(notebook_name)\n",
    "cells = exec_info[len(exec_info)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cells:\n",
    "    if c[\"cell_type\"] == \"code\":\n",
    "        print(c[\"dataframes\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "os.chdir(\"/root/FYP\")\n",
    "\n",
    "from src.execution import get_execution_state\n",
    "from src.prompts import add_new_dataframe_cells\n",
    "\n",
    "with open(os.path.join(\"datasets\", \"dataset.formatted.json\")) as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "ni = 62\n",
    "notebook_name = dataset[ni][\"notebook_name\"]\n",
    "task_index = -1\n",
    "task = dataset[ni][\"turns\"][task_index]\n",
    "exec_info = get_execution_state(notebook_name)\n",
    "cells = exec_info[len(exec_info)-1]\n",
    "\n",
    "from src.prompts import build_task_prompt\n",
    "\n",
    "notebook_prompt_config =  dict(\n",
    "    add_markdown=True,\n",
    "    split_into_cells=True, \n",
    "    add_outputs=False, \n",
    "    add_new_dataframes=False, \n",
    "    append_dataframes=True,\n",
    "    append_schema_info=False,\n",
    "    max_dataframes = 15,\n",
    "    add_exemplars = None,\n",
    "    prioritize_dataframes = True,\n",
    "    max_prompt_size=7000,\n",
    "    max_context_cells=0\n",
    ")\n",
    "\n",
    "# out = build_notebook_prompt(cell_info, **notebook_prompt_config)\n",
    "\n",
    "prompt, out = build_task_prompt(task, cells, notebook_prompt_config)\n",
    "\n",
    "print(prompt[-1][\"content\"])\n",
    "\n",
    "print(\"\\n###Answer###\\n\", task[\"turn\"][\"code\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in task[]:\n",
    "#     if c[\"cell_type\"] == \"code\":\n",
    "#         print(c[\"\"])\n",
    "\n",
    "from src.analysis import analyse_code, get_all_callable_pandas_methods\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "ms = get_all_callable_pandas_methods()\n",
    "\n",
    "def extract_quoted_strings(text):\n",
    "    pattern = r'\\'(.*?)\\'|\\\"(.*?)\\\"'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    quoted_strings = [match[0] if match[0] else match[1] for match in matches]\n",
    "    return quoted_strings\n",
    "\n",
    "\n",
    "# potential_columns = [m for m in res[\"modules\"][\"pandas\"] if m not in ms] + \\\n",
    "#     extract_quoted_strings(code)\n",
    "\n",
    "# used_columns = defaultdict(list)\n",
    "# for col in set(potential_columns):\n",
    "#     for d in set(used_dataframes):\n",
    "#         if d in current_dataframes and col in set(current_dataframes[d].columns):\n",
    "#             used_columns[d].append(col)\n",
    "\n",
    "\n",
    "for i, task in enumerate(dataset[ni][\"turns\"]):\n",
    "    task_exec_info = exec_info[i]\n",
    "\n",
    "    current_dataframes = None\n",
    "    for cell in task_exec_info[::-1]:\n",
    "        if cell[\"cell_type\"] == \"code\":\n",
    "            current_dataframes = cell[\"dataframes\"]\n",
    "            break\n",
    "\n",
    "    code = task[\"turn\"][\"code\"][\"value\"]\n",
    "    code_context = task[\"turn\"][\"code_context\"]\n",
    "    res = analyse_code(code_context, code)\n",
    "    pandas_methods = list(set([m for m in res[\"modules\"][\"pandas\"] if m in ms]))\n",
    "    used_dataframes = list(set([df for df in res[\"used_dataframes\"] if df in current_dataframes]))\n",
    "    used_columns = defaultdict(list)\n",
    "\n",
    "    print(current_dataframes.keys())\n",
    "\n",
    "    for d in set(used_dataframes): \n",
    "        if d in current_dataframes and isinstance(current_dataframes[d], pd.DataFrame):\n",
    "            for col in set(current_dataframes[d].columns):\n",
    "                if col in code:\n",
    "                    used_columns[d].append(col)\n",
    "\n",
    "    pandas_methods_prompt = \"\"\n",
    "    dataframes_prompt = \"\"\n",
    "    if len(pandas_methods) > 1:\n",
    "        pandas_methods_prompt = f\"\"\"# I should use the '{\"', '\".join(pandas_methods[:-1])}' and '{pandas_methods[-1]}' methods.\"\"\"\n",
    "    elif pandas_methods:\n",
    "        pandas_methods_prompt = f\"\"\"# I should use the '{pandas_methods[0]}' method.\"\"\"\n",
    "    if len(used_dataframes) > 1:\n",
    "        dataframes_prompt = f\"\"\"# I should use '{\"', '\".join(used_dataframes[:-1])}' and '{used_dataframes[-1]}'.\"\"\"\n",
    "    elif used_dataframes:\n",
    "        dataframes_prompt = f\"\"\"# I should use '{used_dataframes[0]}'.\"\"\"\n",
    "    for df, cols in used_columns.items():\n",
    "        if len(cols) > 1:\n",
    "            dataframes_prompt += f\"\"\"\\n# I should use the '{\"', '\".join(cols[:-1])}' and '{cols[-1]}' columns from '{df}'.\"\"\"\n",
    "        elif cols:\n",
    "            dataframes_prompt += f\"\"\"\\n# I should use '{cols[0]}' column from '{df}'.\"\"\"\n",
    "    cot_prompt = \"\\n\".join([\"# Step 1:\", dataframes_prompt, pandas_methods_prompt, \"\\n# Step 2:\"]) + \"\\n\"\n",
    "    cell_prompt = cot_prompt + code\n",
    "    print(\"\\n\" + cell_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
